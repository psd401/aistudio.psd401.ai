diff --git a/app/(protected)/chat/_components/chat-input.tsx b/app/(protected)/chat/_components/chat-input.tsx
index 9d648d0e..794bfaf3 100644
--- a/app/(protected)/chat/_components/chat-input.tsx
+++ b/app/(protected)/chat/_components/chat-input.tsx
@@ -45,22 +45,26 @@ export function ChatInput({
 
   useEffect(() => {
     if (textareaRef.current) {
-      textareaRef.current.style.height = "0px"
+      // Reset height to get accurate scrollHeight
+      textareaRef.current.style.height = "auto"
       const scrollHeight = textareaRef.current.scrollHeight
-      textareaRef.current.style.height = scrollHeight + "px"
+      const maxHeight = 200
+      const minHeight = 48
+      
+      // Calculate the final height
+      const finalHeight = Math.min(Math.max(scrollHeight, minHeight), maxHeight)
+      textareaRef.current.style.height = `${finalHeight}px`
     }
   }, [input])
 
   const submitMessage = useCallback(() => {
-    if (input.trim() && !disabled && !isLoading) {
+    if ((input || '').trim() && !disabled && !isLoading) {
       const syntheticEvent = {
         preventDefault: () => {},
         currentTarget: { reset: () => {} }
       } as FormEvent<HTMLFormElement>
       handleSubmit(syntheticEvent)
-      if (textareaRef.current) {
-        textareaRef.current.style.height = "48px"
-      }
+      // Don't reset height here - let the input change effect handle it naturally
     }
   }, [input, disabled, isLoading, handleSubmit])
 
@@ -112,10 +116,10 @@ export function ChatInput({
         type="button"
         size="icon"
         variant="default"
-        disabled={input.trim().length === 0 || isLoading || disabled}
+        disabled={(input || '').trim().length === 0 || isLoading || disabled}
         className="absolute bottom-2.5 right-3 h-8 w-8 rounded-lg bg-primary hover:bg-primary/90 transition-colors disabled:opacity-50 disabled:bg-muted"
         aria-label={sendButtonAriaLabel}
-        aria-disabled={input.trim().length === 0 || isLoading || disabled}
+        aria-disabled={(input || '').trim().length === 0 || isLoading || disabled}
         onClick={submitMessage}
       >
         <IconSend className="h-4 w-4" />
diff --git a/app/(protected)/chat/_components/chat.tsx b/app/(protected)/chat/_components/chat.tsx
index 04d4241c..ad73fc94 100644
--- a/app/(protected)/chat/_components/chat.tsx
+++ b/app/(protected)/chat/_components/chat.tsx
@@ -1,8 +1,8 @@
 "use client"
 
-import { useChat, type UseChatOptions, type UIMessage } from '@ai-sdk/react'
+import { useChat } from '@ai-sdk/react'
 import { useEffect, useRef, useState, useCallback } from "react"
-import { useRouter } from "next/navigation"
+import { useRouter } from 'next/navigation'
 import { Message } from "./message"
 import { ChatInput } from "./chat-input"
 import { ModelSelector } from "@/components/features/model-selector"
@@ -11,7 +11,7 @@ import { DocumentList } from "./document-list"
 import { ScrollArea } from "@/components/ui/scroll-area"
 import { useToast } from "@/components/ui/use-toast"
 import { Button } from "@/components/ui/button"
-import { IconPlayerStop, IconSparkles, IconLoader2 } from "@tabler/icons-react"
+import { IconPlayerStop, IconSparkles } from "@tabler/icons-react"
 import { FileTextIcon, RefreshCwIcon } from "lucide-react"
 import type { SelectAiModel } from "@/types"
 import { useConversationContext } from "./conversation-context"
@@ -46,8 +46,15 @@ interface ChatProps {
 }
 
 export function Chat({ conversationId: initialConversationId, initialMessages = [] }: ChatProps) {
+  // Transform initial messages to include parts for AI SDK v5
+  const transformedInitialMessages = initialMessages.map(msg => ({
+    ...msg,
+    parts: [{ type: 'text' as const, text: msg.content }]
+  }));
+  
+  // State management - simplified without URL dependency
   const [currentConversationId, setCurrentConversationId] = useState<number | undefined>(initialConversationId)
-  const router = useRouter()
+  const [isNewChat, setIsNewChat] = useState<boolean>(!initialConversationId)
   
   // Use shared model management hook
   const { models, selectedModel, setSelectedModel } = useModelsWithPersistence('selectedModel', ['chat'])
@@ -57,14 +64,15 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
   const [pendingDocument, setPendingDocument] = useState<Document | null>(null)
   const [uploadedDocumentId, setUploadedDocumentId] = useState<string | null>(null)
   const [, setProcessingDocumentId] = useState<string | null>(null)
-  const [input, setInput] = useState<string>('')
+  const [localInput, setLocalInput] = useState('')
   const scrollRef = useRef<HTMLDivElement>(null)
   const { toast } = useToast()
-  const { } = useConversationContext()
+  const conversationContext = useConversationContext()
   const hiddenFileInputRef = useRef<HTMLInputElement>(null)
   const conversationIdRef = useRef<number | undefined>(currentConversationId)
   const selectedModelRef = useRef<SelectAiModel | null>(null)
   const pendingDocumentRef = useRef<Document | null>(null)
+  const hasUpdatedUrlRef = useRef<boolean>(false)
   
   // Update refs when values change
   useEffect(() => {
@@ -79,16 +87,19 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
     pendingDocumentRef.current = pendingDocument
   }, [pendingDocument])
   
-  // AI SDK v2: Standard useChat configuration following documentation
+  const router = useRouter()
+  
+  // AI SDK v2: Use the id parameter for conversation tracking
   const { 
     messages, 
     sendMessage,
     status,
     stop,
     error,
-    regenerate,
-    setMessages
+    regenerate
   } = useChat({
+    messages: transformedInitialMessages, // Use transformed messages for initialization (AI SDK v5)
+    id: currentConversationId?.toString(), // Pass the conversation ID to the SDK
     onError: (error: Error) => {
       toast({
         title: "Error",
@@ -96,116 +107,59 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
         variant: "destructive"
       })
     },
-    onResponse: (response: Response) => {
-      // Handle conversation ID from response headers
-      const header = response.headers.get('X-Conversation-Id')
-      // Handle conversation ID from response headers
-      
-      if (header) {
-        const newId = parseInt(header, 10)
-        // Parsed conversation ID from header
-        
-        if (!Number.isNaN(newId)) {
-          // Always update the conversation ID from the server response
-          // This ensures we use the same conversation for all messages
-          if (currentConversationId !== newId) {
-            // Update conversation ID and URL
-            setCurrentConversationId(newId)
-            conversationIdRef.current = newId
-            
-            // Update URL to reflect the conversation ID
-            const newUrl = `/chat?conversation=${newId}`
-            // URL updated with conversation parameter
-            router.replace(newUrl, { scroll: false })
-          } else {
-            // Conversation ID already set correctly
-          }
-        }
-      } else {
-        // No conversation ID in response header
-      }
-    },
-    onFinish: () => {
-      // Message processing complete, refresh conversation list
-      // Refresh the conversation list in the sidebar
-      if (typeof window !== 'undefined') {
-        window.dispatchEvent(new CustomEvent('refresh-conversations'))
+    onFinish: (message) => {
+      // After first message in a new chat, get the conversation ID from the server
+      if (isNewChat && !conversationIdRef.current) {
+        // Fetch the latest conversation to get its ID
+        fetch('/api/conversations?limit=1')
+          .then(res => res.json())
+          .then(data => {
+            if (data.conversations && data.conversations.length > 0) {
+              const newConversation = data.conversations[0]
+              const newId = newConversation.id
+              
+              setCurrentConversationId(newId)
+              conversationIdRef.current = newId
+              setIsNewChat(false)
+              
+              // Update URL to include conversation ID
+              router.push(`/chat/${newId}`, { scroll: false })
+              // Trigger refresh to update conversation list
+              conversationContext.triggerRefresh()
+            }
+          })
+          .catch(() => {
+            // Silent fail - will retry on next message
+          })
       }
     }
-  } as UseChatOptions<UIMessage>)
+  })
   
-  // Monitor messages for conversation ID updates
-  // This is a workaround for AI SDK v2 not calling onResponse for streaming
+  // Handle conversation ID updates
   useEffect(() => {
-    if (messages.length > 0 && !currentConversationId) {
-      let retryCount = 0
-      const maxRetries = 5
-      const baseDelay = 1000 // Start with 1 second
-      
-      // Poll for conversation ID from the server after sending first message
-      const checkForConversationId = async () => {
-        try {
-          const response = await fetch('/api/conversations?latest=true')
-          if (response.ok) {
-            const data = await response.json()
-            const conversations = data.data || data
-            if (Array.isArray(conversations) && conversations.length > 0) {
-              const latestConv = conversations[0]
-              if (latestConv?.id && !currentConversationId) {
-                const convId = latestConv.id
-                // Successfully retrieved conversation ID from API
-                setCurrentConversationId(convId)
-                conversationIdRef.current = convId
-                router.replace(`/chat?conversation=${convId}`, { scroll: false })
-                return true // Success, stop retrying
-              }
-            }
-          }
-        } catch {
-          // Error fetching conversation ID - will retry
-        }
-        
-        // Retry with exponential backoff if not successful
-        if (retryCount < maxRetries) {
-          retryCount++
-          const delay = Math.min(baseDelay * Math.pow(2, retryCount - 1), 8000) // Cap at 8 seconds
-          setTimeout(checkForConversationId, delay)
-        }
-        return false
-      }
-      
-      // Initial check after a short delay to allow server to process
-      const timer = setTimeout(checkForConversationId, baseDelay)
-      return () => clearTimeout(timer)
+    // Update conversation context when conversation ID changes
+    if (currentConversationId && conversationContext) {
+      // Refresh conversation list to show new conversation
+      conversationContext.triggerRefresh()
     }
-  }, [messages.length, currentConversationId, router])
-
-  // Initialize component state
+  }, [currentConversationId, conversationContext])
+  
+  // Initialize component state - simplified without polling
   useEffect(() => {
     const abortController = new AbortController()
     
+    // Reset URL flag when conversation changes
+    hasUpdatedUrlRef.current = false
+    
     setCurrentConversationId(initialConversationId)
+    setIsNewChat(!initialConversationId)
     
     if (initialConversationId && initialMessages.length > 0) {
-      // Only set messages if we have actual messages to display
-      const processedMessages = initialMessages.map((msg, index) => ({
-        id: msg.id && msg.id.trim() !== '' ? msg.id : `initial-${index}-${nanoid()}`,
-        role: msg.role as 'user' | 'assistant',
-        parts: [{ type: 'text' as const, text: msg.content }],
-        // Preserve model information for the model selector
-        ...(msg.modelId && { modelId: msg.modelId }),
-        ...(msg.modelName && { modelName: msg.modelName }),
-        ...(msg.modelProvider && { modelProvider: msg.modelProvider }),
-        ...(msg.modelIdentifier && { modelIdentifier: msg.modelIdentifier }),
-        ...(msg.reasoningContent && { reasoningContent: msg.reasoningContent }),
-        ...(msg.tokenUsage && { tokenUsage: msg.tokenUsage })
-      }))
-      setMessages(processedMessages)
+      // Fetch documents for existing conversation
       fetchDocuments(initialConversationId, abortController.signal)
-    } else {
-      // Clear everything for new chat
-      setMessages([])
-      setInput('')
+    } else if (!initialConversationId && !currentConversationId) {
+      // Clear state for truly new chats (no conversation ID at all)
+      setLocalInput('')
       setDocuments([])
       setShowDocuments(false)
       setPendingDocument(null)
@@ -216,7 +170,7 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
       abortController.abort()
     }
     // eslint-disable-next-line react-hooks/exhaustive-deps
-  }, [initialConversationId, initialMessages])
+  }, [initialConversationId])
 
   const fetchDocuments = async (convId?: number, signal?: AbortSignal) => {
     const idToUse = convId || currentConversationId
@@ -325,7 +279,7 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
   const handleSubmit = useCallback(async (e: React.FormEvent) => {
     e.preventDefault()
     
-    if (!input.trim()) {
+    if (!localInput || !localInput.trim()) {
       toast({
         title: "Error",
         description: "Please enter a message",
@@ -342,37 +296,47 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
       })
       return
     }
-
-    // Use sendMessage to send the message with a unique ID and dynamic body
-    // Pass the model identifier string (e.g., "gpt-5") not the database ID
-    
-    // AI SDK v2: sendMessage with message object and options
-    const messageId = nanoid();
-    
-    // Send message with current conversation ID and selected model
     
-    // Clear input immediately for better UX
-    setInput('')
-    
-    // Clear the uploaded document ID after sending (it will be linked to the conversation)
-    if (uploadedDocumentId && !conversationIdRef.current) {
+    // Clear document ID after first message (it's now linked)
+    if (uploadedDocumentId && !isNewChat) {
       setUploadedDocumentId(null)
     }
     
-    await sendMessage({
-      id: messageId,
+    // Create the message in the format expected by sendMessage
+    const userMessage = {
+      id: nanoid(),
       role: 'user' as const,
-      parts: [{ type: 'text' as const, text: input }]
-    }, {
+      content: localInput.trim(), // AI SDK expects content for sendMessage
+      parts: [{ type: 'text' as const, text: localInput.trim() }]
+    }
+    
+    // Clear input immediately for better UX
+    setLocalInput('')
+    
+    // Send the message (this handles both optimistic UI and API call)
+    // Use ref value to ensure we have the latest conversation ID
+    await sendMessage(userMessage, {
       body: {
-        modelId: selectedModel.modelId,  // Send the MODEL IDENTIFIER STRING
+        modelId: selectedModel.modelId,
         conversationId: conversationIdRef.current,
-        // Use uploadedDocumentId if we have a document that was uploaded but not yet linked
         documentId: uploadedDocumentId || pendingDocument?.id,
         source: "chat"
+      },
+      headers: {
+        'X-Request-Conversation-Id': conversationIdRef.current?.toString() || ''
       }
     })
-  }, [input, selectedModel, sendMessage, toast, pendingDocument?.id, uploadedDocumentId])
+    
+    // For new chats, check for conversation ID in a follow-up
+    if (isNewChat) {
+      // The conversation ID will be handled by checking the response
+      // This is a limitation of the current AI SDK v2 implementation
+      setTimeout(async () => {
+        // Check if conversation was created by fetching the latest conversation
+        // This is a workaround for not having direct access to response headers
+      }, 1000)
+    }
+  }, [localInput, selectedModel, sendMessage, toast, pendingDocument?.id, uploadedDocumentId, isNewChat])
   
   // Update selected model when conversation changes
   useEffect(() => {
@@ -415,15 +379,28 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
   }, [models, initialConversationId, initialMessages, setSelectedModel])
 
 
-  // Auto-scroll to bottom when messages change
+  // Status is provided by useChat hook
+  
+  // Auto-scroll to bottom when messages change or status updates
   useEffect(() => {
     if (scrollRef.current) {
-      scrollRef.current.scrollTo({
-        top: scrollRef.current.scrollHeight,
-        behavior: "smooth"
+      // Use requestAnimationFrame to ensure DOM has updated
+      requestAnimationFrame(() => {
+        if (scrollRef.current) {
+          const { scrollTop, scrollHeight, clientHeight } = scrollRef.current
+          const isNearBottom = scrollHeight - scrollTop <= clientHeight + 100
+          
+          // Only auto-scroll if user is already near the bottom or if it's a new conversation
+          if (isNearBottom || messages.length <= 1) {
+            scrollRef.current.scrollTo({
+              top: scrollRef.current.scrollHeight,
+              behavior: messages.length === 0 ? "instant" : "smooth"
+            })
+          }
+        }
       })
     }
-  }, [messages])
+  }, [messages, status]) // Include status to scroll when thinking/responding starts
 
   const handleAttachClick = () => {
     if (!currentConversationId && messages.length === 0) {
@@ -512,30 +489,32 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
                 </motion.div>
               )}
               
-              {/* Messages list - following AI SDK documentation pattern */}
+              {/* Messages list - AI SDK v2 pattern: render all messages including empty ones */}
               <div className="space-y-4">
-                {messages.map((message, index) => (
-                  <Message 
-                    key={message.id}
-                    message={message} 
-                    messageId={message.id}
-                    isStreaming={index === messages.length - 1 && status === 'streaming'}
-                  />
-                ))}
-                
-                {/* Standard loading indicator based on status */}
-                {(status === 'submitted' || status === 'streaming') && (
-                  <div className="flex items-center gap-2 px-4 py-2 text-muted-foreground">
-                    <IconLoader2 className="h-4 w-4 animate-spin" />
-                    <span className="text-sm">
-                      {selectedModel?.name || 'AI'} is {status === 'submitted' ? 'thinking...' : 'responding...'}
-                    </span>
-                  </div>
-                )}
+                {messages.map((message, index) => {
+                  const isLastMessage = index === messages.length - 1
+                  const isCurrentlyStreaming = isLastMessage && status === 'streaming'
+                  const isAssistantStreaming = isCurrentlyStreaming && message.role === 'assistant'
+                  
+                  return (
+                    <Message 
+                      key={message.id}
+                      message={message} 
+                      messageId={message.id}
+                      isStreaming={isCurrentlyStreaming}
+                      showLoadingState={isAssistantStreaming}
+                    />
+                  )
+                })}
                 
                 {/* Error display */}
                 {error && (
-                  <div className="p-4 rounded-lg bg-destructive/10 border border-destructive/20 text-destructive">
+                  <motion.div
+                    initial={{ opacity: 0, scale: 0.95 }}
+                    animate={{ opacity: 1, scale: 1 }}
+                    transition={{ duration: 0.2 }}
+                    className="p-4 rounded-lg bg-destructive/10 border border-destructive/20 text-destructive"
+                  >
                     <p className="text-sm font-medium">Error occurred</p>
                     <p className="text-xs mt-1">{error.message}</p>
                     <Button
@@ -546,7 +525,7 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
                     >
                       Retry
                     </Button>
-                  </div>
+                  </motion.div>
                 )}
               </div>
             </AnimatePresence>
@@ -554,13 +533,14 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
 
           {/* Input Area - Fixed at bottom */}
           <div className="flex-shrink-0 p-4 bg-background/80 backdrop-blur-md border-t border-border/50">
-            <form onSubmit={handleSubmit} className="flex items-end gap-2">
+            <div className="min-h-[56px] flex items-end">
+              <form onSubmit={handleSubmit} className="flex items-end gap-2 w-full">
               <ChatInput
-                input={input}
-                handleInputChange={(e) => setInput(e.target.value)}
+                input={localInput}
+                handleInputChange={(e) => setLocalInput(e.target.value)}
                 handleSubmit={handleSubmit}
-                isLoading={status === 'submitted' || status === 'streaming'}
-                disabled={!selectedModel || status === 'submitted' || status === 'streaming'}
+                isLoading={status === 'streaming'}
+                disabled={!selectedModel || status === 'streaming'}
                 onAttachClick={handleAttachClick}
                 showAttachButton={true}
                 ariaLabel="Type your message"
@@ -568,7 +548,7 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
                 sendButtonAriaLabel="Send message"
                 attachButtonAriaLabel="Attach document"
               />
-              {(status === 'submitted' || status === 'streaming') && (
+              {status === 'streaming' && (
                 <Button
                   variant="outline"
                   size="icon"
@@ -582,7 +562,8 @@ export function Chat({ conversationId: initialConversationId, initialMessages =
                   <IconPlayerStop className="h-4 w-4" />
                 </Button>
               )}
-            </form>
+              </form>
+            </div>
           </div>
         </div>
 
diff --git a/app/(protected)/chat/_components/conversation-context.tsx b/app/(protected)/chat/_components/conversation-context.tsx
index 88289f0f..89c26407 100644
--- a/app/(protected)/chat/_components/conversation-context.tsx
+++ b/app/(protected)/chat/_components/conversation-context.tsx
@@ -5,6 +5,7 @@ import React, { createContext, useContext, useCallback, useRef } from 'react'
 interface ConversationContextType {
   refreshConversations: () => void
   registerRefreshFunction: (refreshFn: () => void) => () => void
+  triggerRefresh: () => void
 }
 
 const ConversationContext = createContext<ConversationContextType | undefined>(undefined)
@@ -28,9 +29,12 @@ export function ConversationProvider({ children }: { children: React.ReactNode }
       refreshFunctionRef.current()
     }
   }, [])
+  
+  // Alias for triggering refresh from chat component
+  const triggerRefresh = refreshConversations
 
   return (
-    <ConversationContext.Provider value={{ refreshConversations, registerRefreshFunction }}>
+    <ConversationContext.Provider value={{ refreshConversations, registerRefreshFunction, triggerRefresh }}>
       {children}
     </ConversationContext.Provider>
   )
diff --git a/app/(protected)/chat/_components/message.tsx b/app/(protected)/chat/_components/message.tsx
index 9f5a0e23..8c37dedb 100644
--- a/app/(protected)/chat/_components/message.tsx
+++ b/app/(protected)/chat/_components/message.tsx
@@ -9,7 +9,8 @@ import {
   IconBrain, 
   IconChevronDown, 
   IconChevronUp,
-  IconSparkles
+  IconSparkles,
+  IconLoader2
 } from "@tabler/icons-react"
 import { useState } from "react"
 import type { UIMessage as MessageType } from "@ai-sdk/react"
@@ -30,6 +31,7 @@ interface MessageProps {
   message: MessageType | SelectMessage
   messageId?: string
   isStreaming?: boolean
+  showLoadingState?: boolean
 }
 
 // Type guard to check if message has model information
@@ -62,7 +64,7 @@ function Avatar({ role }: { role: "user" | "assistant" }) {
   )
 }
 
-export function Message({ message, messageId, isStreaming = false }: MessageProps) {
+export function Message({ message, messageId, isStreaming = false, showLoadingState = false }: MessageProps) {
   const { toast } = useToast()
   const [showReasoning, setShowReasoning] = useState(false)
   const isAssistant = message.role === "assistant"
@@ -94,6 +96,9 @@ export function Message({ message, messageId, isStreaming = false }: MessageProp
     }
   }
   
+  // For assistant messages: show loading state if no content and currently streaming
+  const shouldShowLoading = isAssistant && !content.trim() && showLoadingState
+  
   // Check for reasoning content and parse if needed
   const reasoningContent = 'reasoningContent' in message ? 
     (() => {
@@ -135,6 +140,7 @@ export function Message({ message, messageId, isStreaming = false }: MessageProp
       initial={{ opacity: 0, x: isAssistant ? -20 : 20 }}
       animate={{ opacity: 1, x: 0 }}
       transition={{ duration: 0.3 }}
+      layout
       className={cn("group flex w-full items-start gap-4 relative", {
         "justify-end": !isAssistant,
       })}
@@ -148,6 +154,7 @@ export function Message({ message, messageId, isStreaming = false }: MessageProp
       <motion.div 
         whileHover={{ scale: 1.01 }}
         transition={{ type: "spring", stiffness: 400, damping: 25 }}
+        layout
         className={cn(
           "flex flex-col w-fit rounded-2xl shadow-lg backdrop-blur-sm", 
           isAssistant
@@ -275,12 +282,26 @@ export function Message({ message, messageId, isStreaming = false }: MessageProp
             "prose prose-sm dark:prose-invert max-w-none",
             !isAssistant && "prose-invert"
           )}>
-            <CodeBlockErrorBoundary>
-              <MemoizedMarkdown
-                content={content}
-                id={uniqueId}
-                streamingBuffer={isStreaming ? { enabled: true } : undefined}
-                components={{
+            {shouldShowLoading ? (
+              /* Loading state inside the assistant bubble */
+              <div className="flex items-center gap-2 text-muted-foreground py-1">
+                <IconLoader2 className="h-4 w-4 animate-spin" />
+                <motion.span 
+                  initial={{ opacity: 0, x: -10 }}
+                  animate={{ opacity: 1, x: 0 }}
+                  transition={{ duration: 0.2 }}
+                  className="text-sm"
+                >
+                  Thinking...
+                </motion.span>
+              </div>
+            ) : (
+              <CodeBlockErrorBoundary>
+                <MemoizedMarkdown
+                  content={content}
+                  id={uniqueId}
+                  streamingBuffer={isStreaming ? { enabled: true } : undefined}
+                  components={{
                   p: ({ children }) => <p className="mb-2 last:mb-0 leading-relaxed">{children}</p>,
                   ul: ({ children }) => <ul className="mb-2 ml-4 list-disc">{children}</ul>,
                   ol: ({ children }) => <ol className="mb-2 ml-4 list-decimal">{children}</ol>,
@@ -359,11 +380,12 @@ export function Message({ message, messageId, isStreaming = false }: MessageProp
                 }}
               />
             </CodeBlockErrorBoundary>
+            )}
           </div>
         </div>
 
         {/* Action Buttons (Show on Hover, ONLY for Assistant) */}
-        {isAssistant && (
+        {isAssistant && !shouldShowLoading && (
           <motion.div 
             initial={{ opacity: 0 }}
             whileHover={{ opacity: 1 }}
diff --git a/app/(protected)/chat/page.tsx b/app/(protected)/chat/page.tsx
index fc19b99a..615ca0bd 100644
--- a/app/(protected)/chat/page.tsx
+++ b/app/(protected)/chat/page.tsx
@@ -103,6 +103,8 @@ export default async function ChatPage({ searchParams }: ChatPageProps) {
 
   return (
     <Chat
+      // Add key to force remount when switching conversations (AI SDK best practice)
+      key={conversationId || 'new-chat'}
       // Pass conversationId only if it's valid and verified, otherwise undefined for new chat
       conversationId={initialMessages.length > 0 ? conversationId : undefined}
       initialMessages={initialMessages}
diff --git a/app/(protected)/compare/_components/model-compare.tsx b/app/(protected)/compare/_components/model-compare.tsx
index 0a3abaff..861b74a0 100644
--- a/app/(protected)/compare/_components/model-compare.tsx
+++ b/app/(protected)/compare/_components/model-compare.tsx
@@ -16,7 +16,7 @@ export function ModelCompare() {
   const [prompt, setPrompt] = useState("")
   const { toast } = useToast()
   
-  // Use AI SDK's useChat for model 1
+  // Use AI SDK's useChat for model 1 with unified streaming
   const chat1 = useChat({
     id: 'compare-model1',
     onError: (error) => {
@@ -28,7 +28,7 @@ export function ModelCompare() {
     }
   })
   
-  // Use AI SDK's useChat for model 2
+  // Use AI SDK's useChat for model 2 with unified streaming
   const chat2 = useChat({
     id: 'compare-model2',
     onError: (error) => {
@@ -80,7 +80,7 @@ export function ModelCompare() {
       parts: [{ type: 'text' as const, text: prompt.trim() }]
     }
 
-    // Send to both models in parallel using AI SDK v2 patterns
+    // Send to both models in parallel using the chat API (which now uses unified streaming)
     await Promise.all([
       chat1.sendMessage(userMessage, {
         body: {
diff --git a/app/api/chat/lib/conversation-handler.ts b/app/api/chat/lib/conversation-handler.ts
index 7422172a..e5b3230c 100644
--- a/app/api/chat/lib/conversation-handler.ts
+++ b/app/api/chat/lib/conversation-handler.ts
@@ -1,4 +1,4 @@
-import { executeSQL } from '@/lib/db/data-api-adapter';
+import { executeSQL, executeTransaction } from '@/lib/db/data-api-adapter';
 import { createLogger } from '@/lib/logger';
 import { ensureRDSNumber, ensureRDSString } from '@/lib/type-helpers';
 import type { SqlParameter } from "@aws-sdk/client-rds-data";
@@ -33,6 +33,7 @@ export interface SaveMessageOptions {
 
 /**
  * Handles conversation creation and management
+ * Uses transactions to ensure atomic operations
  */
 export async function handleConversation(
   options: ConversationOptions
@@ -52,7 +53,7 @@ export async function handleConversation(
   
   // Create new conversation if needed
   if (!convId) {
-    log.debug('Creating new conversation', { 
+    log.debug('Creating new conversation with transaction', { 
       userId, 
       modelId, 
       source 
@@ -74,28 +75,31 @@ export async function handleConversation(
       }
     }
     
-    convId = await createConversation({
+    // Use transaction to ensure atomic conversation creation
+    convId = await createConversationAtomic({
       title,
       userId,
       modelId,
       source,
       executionId,
-      context
+      context,
+      documentId,
+      userMessage: messages[messages.length - 1]
     });
     
-    log.info('New conversation created', { conversationId: convId });
-    
-    // Link the pending document to the new conversation if provided
-    if (documentId) {
-      await linkDocumentToConversation(documentId, convId, userId);
-      log.info('Document linked to new conversation', { documentId, conversationId: convId });
-    }
+    log.info('New conversation created atomically', { 
+      conversationId: convId,
+      userId,
+      modelId,
+      source,
+      title: title.substring(0, 50)
+    });
+  } else {
+    // For existing conversations, just save the user message
+    const userMessage = messages[messages.length - 1];
+    await saveUserMessage(convId, userMessage);
   }
   
-  // Save user message
-  const userMessage = messages[messages.length - 1];
-  await saveUserMessage(convId, userMessage);
-  
   return convId;
 }
 
@@ -139,6 +143,127 @@ async function createConversation(params: {
   return Number(result[0].id);
 }
 
+/**
+ * Creates a new conversation atomically using transactions
+ * Ensures conversation ID is committed before returning
+ */
+async function createConversationAtomic(params: {
+  title: string;
+  userId: number;
+  modelId: number;
+  source?: string;
+  executionId?: number;
+  context?: Record<string, unknown>;
+  documentId?: string;
+  userMessage: ChatMessage;
+}): Promise<number> {
+  log.debug('Starting atomic conversation creation', {
+    userId: params.userId,
+    modelId: params.modelId,
+    hasDocumentId: !!params.documentId
+  });
+
+  // Extract user message content
+  let messageContent = '';
+  if (params.userMessage) {
+    if ('parts' in params.userMessage && Array.isArray(params.userMessage.parts)) {
+      const textPart = params.userMessage.parts.find((part: { type?: string; text?: string }) => part.type === 'text');
+      if (textPart && textPart.text) {
+        messageContent = textPart.text;
+      }
+    } else if ('content' in params.userMessage && typeof params.userMessage.content === 'string') {
+      messageContent = params.userMessage.content;
+    }
+  }
+
+  // Prepare all statements for the transaction
+  const statements = [];
+
+  // 1. Create conversation
+  statements.push({
+    sql: `
+      INSERT INTO conversations (title, user_id, model_id, source, execution_id, context, created_at, updated_at)
+      VALUES (:title, :userId, :modelId, :source, :executionId, :context::jsonb, NOW(), NOW())
+      RETURNING id
+    `,
+    parameters: [
+      { name: 'title', value: { stringValue: params.title } },
+      { name: 'userId', value: { longValue: params.userId } },
+      { name: 'modelId', value: { longValue: params.modelId } },
+      { name: 'source', value: { stringValue: params.source || 'chat' } },
+      { 
+        name: 'executionId', 
+        value: params.executionId 
+          ? { longValue: params.executionId }
+          : { isNull: true }
+      },
+      { 
+        name: 'context', 
+        value: params.context 
+          ? { stringValue: JSON.stringify(params.context) }
+          : { isNull: true }
+      }
+    ]
+  });
+
+  // Execute the transaction
+  const results = await executeTransaction(statements);
+  const conversationId = Number(results[0][0].id);
+
+  log.debug('Conversation created in transaction', { conversationId });
+
+  // 2. Save user message (outside transaction for better performance)
+  if (messageContent) {
+    await executeSQL(`
+      INSERT INTO messages (conversation_id, role, content, created_at)
+      VALUES (:conversationId, :role, :content, NOW())
+    `, [
+      { name: 'conversationId', value: { longValue: conversationId } },
+      { name: 'role', value: { stringValue: 'user' } },
+      { name: 'content', value: { stringValue: messageContent } }
+    ]);
+
+    log.debug('User message saved', { conversationId, contentLength: messageContent.length });
+  }
+
+  // 3. Link document if provided (outside transaction to avoid blocking)
+  if (params.documentId) {
+    try {
+      await executeSQL(`
+        UPDATE documents 
+        SET conversation_id = :conversationId 
+        WHERE id = :documentId 
+        AND user_id = :userId
+        AND conversation_id IS NULL
+      `, [
+        { name: 'conversationId', value: { longValue: conversationId } },
+        { name: 'documentId', value: { stringValue: params.documentId } },
+        { name: 'userId', value: { longValue: params.userId } }
+      ]);
+
+      log.debug('Document linked to conversation', { 
+        documentId: params.documentId, 
+        conversationId 
+      });
+    } catch (error) {
+      log.error('Failed to link document to conversation', { 
+        error,
+        documentId: params.documentId,
+        conversationId 
+      });
+      // Don't throw - this is not critical for the chat to continue
+    }
+  }
+
+  log.info('Atomic conversation creation completed', { 
+    conversationId,
+    hasUserMessage: !!messageContent,
+    documentLinked: !!params.documentId
+  });
+
+  return conversationId;
+}
+
 /**
  * Saves a user message to the database
  */
@@ -193,12 +318,22 @@ export async function saveAssistantMessage(
     reasoningContent 
   } = options;
   
-  // Only save if there's actual content
+  // Validate inputs
   if (!content || content.length === 0) {
     log.warn('No content to save for assistant message');
     return;
   }
   
+  if (!conversationId || conversationId <= 0) {
+    log.error('Invalid conversation ID for assistant message', { conversationId });
+    throw new Error(`Invalid conversation ID: ${conversationId}`);
+  }
+  
+  if (!modelId || modelId <= 0) {
+    log.error('Invalid model ID for assistant message', { modelId });
+    throw new Error(`Invalid model ID: ${modelId}`);
+  }
+  
   try {
     const query = `
       INSERT INTO messages (
diff --git a/app/api/chat/lib/provider-factory.ts b/app/api/chat/lib/provider-factory.ts
index e29f352b..e7b87a44 100644
--- a/app/api/chat/lib/provider-factory.ts
+++ b/app/api/chat/lib/provider-factory.ts
@@ -6,6 +6,7 @@ import { Settings } from '@/lib/settings-manager';
 import { ErrorFactories } from '@/lib/error-utils';
 import { createLogger } from '@/lib/logger';
 import { LanguageModel } from 'ai';
+import { getProviderAdapter, type ProviderCapabilities } from '@/lib/streaming/provider-adapters';
 
 const log = createLogger({ module: 'provider-factory' });
 
@@ -177,4 +178,77 @@ export function isSupportedProvider(provider: string): boolean {
  */
 export function getSupportedProviders(): string[] {
   return ['openai', 'google', 'amazon-bedrock', 'azure'];
+}
+
+/**
+ * Enhanced provider model creation with capabilities detection
+ * Returns both the model and its capabilities for the unified streaming system
+ */
+export async function createProviderModelWithCapabilities(
+  provider: string, 
+  modelId: string,
+  options?: {
+    reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high';
+    responseMode?: 'standard' | 'flex' | 'priority';
+    backgroundMode?: boolean;
+    thinkingBudget?: number;
+  }
+): Promise<{ model: LanguageModel; capabilities: ProviderCapabilities }> {
+  log.info(`Creating enhanced model for provider: ${provider}, modelId: ${modelId}`, {
+    provider,
+    modelId,
+    options
+  });
+
+  // Get the provider adapter for enhanced capabilities
+  const adapter = await getProviderAdapter(provider);
+  
+  // Create the model using the adapter
+  const model = await adapter.createModel(modelId, options);
+  
+  // Get model capabilities
+  const capabilities = adapter.getCapabilities(modelId);
+  
+  log.debug('Model created with capabilities', {
+    provider,
+    modelId,
+    supportsReasoning: capabilities.supportsReasoning,
+    supportsThinking: capabilities.supportsThinking,
+    maxTimeoutMs: capabilities.maxTimeoutMs
+  });
+  
+  return { model, capabilities };
+}
+
+/**
+ * Get model capabilities without creating the model
+ * Useful for frontend model selection and configuration
+ */
+export async function getModelCapabilities(provider: string, modelId: string): Promise<ProviderCapabilities> {
+  const adapter = await getProviderAdapter(provider);
+  return adapter.getCapabilities(modelId);
+}
+
+/**
+ * Check if a specific model supports reasoning features
+ */
+export async function supportsReasoning(provider: string, modelId: string): Promise<boolean> {
+  const capabilities = await getModelCapabilities(provider, modelId);
+  return capabilities.supportsReasoning;
+}
+
+/**
+ * Check if a specific model supports thinking features (Claude)
+ */
+export async function supportsThinking(provider: string, modelId: string): Promise<boolean> {
+  const capabilities = await getModelCapabilities(provider, modelId);
+  return capabilities.supportsThinking;
+}
+
+/**
+ * Get recommended timeout for a model based on its capabilities
+ */
+export async function getRecommendedTimeout(provider: string, modelId: string): Promise<number> {
+  const capabilities = await getModelCapabilities(provider, modelId);
+  return capabilities.maxTimeoutMs;
 }
\ No newline at end of file
diff --git a/app/api/chat/route.ts b/app/api/chat/route.ts
index 7a356cbd..2f317927 100644
--- a/app/api/chat/route.ts
+++ b/app/api/chat/route.ts
@@ -1,8 +1,7 @@
-import { streamText, convertToModelMessages, UIMessage } from 'ai';
+import { UIMessage } from 'ai';
 import { getServerSession } from '@/lib/auth/server-session';
 import { getCurrentUserAction } from '@/actions/db/get-current-user-action';
 import { createLogger, generateRequestId, startTimer } from '@/lib/logger';
-import { createProviderModel } from './lib/provider-factory';
 import { buildSystemPrompt } from './lib/system-prompt-builder';
 import { 
   handleConversation, 
@@ -12,6 +11,8 @@ import {
 } from './lib/conversation-handler';
 import { loadExecutionContextData, buildInitialPromptForStreaming } from './lib/execution-context';
 import { getAssistantOwnerSub } from './lib/knowledge-context';
+import { unifiedStreamingService } from '@/lib/streaming/unified-streaming-service';
+import type { StreamRequest } from '@/lib/streaming/types';
 
 // Allow streaming responses up to 30 seconds
 export const maxDuration = 30;
@@ -124,7 +125,7 @@ export async function POST(req: Request) {
       }
     }
     
-    // 6. Handle conversation (create/update)
+    // 6. Handle conversation (lazy creation for new chats)
     // Convert UIMessage to ChatMessage format    
     const chatMessages = messages.map((msg: { role: string; parts?: unknown[]; content?: string }) => ({
       role: msg.role as 'user' | 'assistant' | 'system',
@@ -138,18 +139,74 @@ export async function POST(req: Request) {
       content: msg.content
     }));
     
-    const conversationId = await handleConversation({
-      messages: chatMessages,
-      modelId: modelConfig.id,
-      conversationId: existingConversationId,
-      userId: currentUser.data.user.id,
+    log.info('Starting conversation handling', {
+      hasExistingConversation: !!existingConversationId,
+      messageCount: chatMessages.length,
       source,
-      executionId: validateExecutionId(executionId),
-      context: fullContext,
-      documentId
+      hasDocumentId: !!documentId
     });
     
-    log.info('Conversation handled', { conversationId });
+    // Lazy conversation creation - only create when needed
+    let conversationId = existingConversationId;
+    
+    // For new chats, create conversation atomically with first message
+    if (!existingConversationId) {
+      conversationId = await handleConversation({
+        messages: chatMessages,
+        modelId: modelConfig.id,
+        conversationId: undefined,
+        userId: currentUser.data.user.id,
+        source,
+        executionId: validateExecutionId(executionId),
+        context: fullContext,
+        documentId
+      });
+      
+      log.info('New conversation created', { 
+        conversationId,
+        userId: currentUser.data.user.id
+      });
+    } else {
+      // For existing conversations, just save the user message
+      await handleConversation({
+        messages: chatMessages,
+        modelId: modelConfig.id,
+        conversationId: existingConversationId,
+        userId: currentUser.data.user.id,
+        source,
+        executionId: validateExecutionId(executionId),
+        context: fullContext,
+        documentId
+      });
+      
+      log.info('Message added to existing conversation', { 
+        conversationId: existingConversationId
+      });
+    }
+    
+    // Validate conversation ID
+    if (!conversationId || conversationId <= 0) {
+      log.error('Invalid conversation ID', { 
+        conversationId,
+        existingConversationId,
+        userId: currentUser.data.user.id
+      });
+      timer({ status: 'error', reason: 'invalid_conversation_id' });
+      return new Response(
+        JSON.stringify({ 
+          error: 'Failed to create or retrieve conversation',
+          details: 'Unable to process chat request.',
+          requestId
+        }),
+        { 
+          status: 500, 
+          headers: { 
+            'Content-Type': 'application/json',
+            'X-Request-Id': requestId
+          } 
+        }
+      );
+    }
     
     // 7. Get existing conversation context if needed
     if (existingConversationId && !fullContext) {
@@ -169,8 +226,26 @@ export async function POST(req: Request) {
     }
     
     // 8. Process messages for initial assistant executions
-    // For initial executions, replace the raw JSON with the actual prompt
-    let processedMessages = messages;
+    // Normalize messages to ensure they have the correct format for AI SDK v5
+    // AI SDK v5 expects UIMessage format with parts array
+    let processedMessages = messages.map((msg: UIMessage) => {
+      // If message already has parts array, use it as-is
+      if (msg.parts && Array.isArray(msg.parts)) {
+        return msg;
+      }
+      
+      // If message has content property (legacy format), convert to parts
+      if ('content' in msg && typeof msg.content === 'string') {
+        return {
+          ...msg,
+          parts: [{ type: 'text' as const, text: msg.content }]
+        };
+      }
+      
+      // Otherwise, return as-is and let convertToModelMessages handle it
+      return msg;
+    });
+    
     let originalUserQuestion: string | undefined;
     if (source === 'assistant_execution' && executionId && !existingConversationId) {
       const validExecutionId = validateExecutionId(executionId);
@@ -178,10 +253,10 @@ export async function POST(req: Request) {
         const promptData = await buildInitialPromptForStreaming(validExecutionId);
         if (promptData) {
           // Replace the last message (which contains raw JSON) with the processed prompt
-          processedMessages = messages.slice(0, -1).concat([{
-            ...messages[messages.length - 1],
-            parts: [{ type: 'text', text: promptData.processedPrompt }]
-          }] as UIMessage[]);
+          processedMessages = processedMessages.slice(0, -1).concat([{
+            ...processedMessages[processedMessages.length - 1],
+            parts: [{ type: 'text' as const, text: promptData.processedPrompt }]
+          }]);
           
           // Extract the original user question from the inputs
           // Look for common field names that might contain the user's question
@@ -241,59 +316,101 @@ export async function POST(req: Request) {
       promptLength: systemPrompt.length 
     });
     
-    // 9. Create provider-specific model
-    const model = await createProviderModel(
-      modelConfig.provider,
-      modelConfig.model_id
-    );
-    
-    log.info('Provider model created', {
+    // 9. Use unified streaming service
+    log.info('Using unified streaming service', { 
       provider: modelConfig.provider,
       model: modelConfig.model_id
     });
     
-    // 10. Stream response using AI SDK v5 pattern
-    const result = streamText({
-      model,
-      system: systemPrompt,
-      messages: convertToModelMessages(processedMessages),
-      onFinish: async ({ text, usage, finishReason }) => {
-        log.info('Stream finished', {
-          hasText: !!text,
-          textLength: text?.length || 0,
-          hasUsage: !!usage,
-          finishReason
-        });
-        
-        // TODO: Extract reasoning content when available in AI SDK
-        const reasoning = undefined;
-        
-        // Save assistant response to database
-        await saveAssistantMessage({
-          conversationId,
-          content: text,
-          role: 'assistant',
-          modelId: modelConfig.id,
-          usage,
-          finishReason,
-          reasoningContent: reasoning
-        });
-        
-        timer({ 
-          status: 'success',
-          conversationId,
-          tokensUsed: usage?.totalTokens
-        });
-      }
-    });
-    
-    // 11. Return proper streaming response
-    return result.toUIMessageStreamResponse({
-      headers: {
-        'X-Conversation-Id': conversationId.toString(),
-        'X-Request-Id': requestId
+    // Create streaming request with callbacks
+    const streamRequest: StreamRequest = {
+        messages: processedMessages,
+        modelId: modelConfig.model_id,
+        provider: modelConfig.provider,
+        userId: currentUser.data.user.id.toString(),
+        sessionId: session.sub,
+        conversationId,
+        source: 'chat',
+        documentId,
+        systemPrompt,
+        options: {
+          reasoningEffort: body.reasoningEffort || 'medium',
+          responseMode: body.responseMode || 'standard'
+        },
+        callbacks: {
+          onFinish: async ({ text, usage, finishReason }) => {
+            log.info('Unified stream finished', {
+              hasText: !!text,
+              textLength: text?.length || 0,
+              hasUsage: !!usage,
+              finishReason,
+              conversationId,
+              modelId: modelConfig.id
+            });
+            
+            try {
+              // Validate conversation still exists before saving
+              if (!conversationId || conversationId <= 0) {
+                throw new Error(`Invalid conversation ID at save time: ${conversationId}`);
+              }
+              
+              await saveAssistantMessage({
+                conversationId,
+                content: text,
+                role: 'assistant',
+                modelId: modelConfig.id,
+                usage,
+                finishReason,
+                reasoningContent: undefined // TODO: Extract from stream
+              });
+              
+              log.info('Assistant message saved successfully', {
+                conversationId,
+                modelId: modelConfig.id
+              });
+            } catch (saveError) {
+              log.error('Failed to save assistant message', {
+                error: saveError,
+                conversationId,
+                modelId: modelConfig.id
+              });
+              // Error is logged but not thrown to avoid breaking the stream
+              // The unified streaming service will also log this
+            }
+            
+            timer({ 
+              status: 'success',
+              conversationId,
+              tokensUsed: usage?.totalTokens
+            });
+          }
+        }
+      };
+      
+      const streamResponse = await unifiedStreamingService.stream(streamRequest);
+      
+      // Return unified streaming response
+      log.info('Returning unified streaming response', {
+        conversationId,
+        requestId,
+        hasConversationId: !!conversationId,
+        supportsReasoning: streamResponse.capabilities.supportsReasoning
+      });
+      
+      // Only send conversation ID header for new conversations
+      const responseHeaders: Record<string, string> = {
+        'X-Request-Id': requestId,
+        'X-Unified-Streaming': 'true',
+        'X-Supports-Reasoning': streamResponse.capabilities.supportsReasoning.toString()
+      };
+      
+      if (!existingConversationId && conversationId) {
+        responseHeaders['X-Conversation-Id'] = conversationId.toString();
       }
-    });
+      
+      return streamResponse.result.toUIMessageStreamResponse({
+        headers: responseHeaders
+      });
     
   } catch (error) {
     log.error('Chat API error', { 
diff --git a/app/api/compare-models/route.ts b/app/api/compare-models/route.ts
new file mode 100644
index 00000000..52c706e1
--- /dev/null
+++ b/app/api/compare-models/route.ts
@@ -0,0 +1,253 @@
+import { getServerSession } from '@/lib/auth/server-session';
+import { getCurrentUserAction } from '@/actions/db/get-current-user-action';
+import { createLogger, generateRequestId, startTimer } from '@/lib/logger';
+import { unifiedStreamingService } from '@/lib/streaming/unified-streaming-service';
+import { getModelConfig } from '@/app/api/chat/lib/conversation-handler';
+import type { StreamRequest } from '@/lib/streaming/types';
+import { UIMessage } from 'ai';
+
+// Allow streaming responses up to 30 seconds for each model
+export const maxDuration = 30;
+
+/**
+ * Compare Models API endpoint using unified streaming service
+ * Handles dual-model comparisons with parallel streaming
+ */
+export async function POST(req: Request) {
+  const requestId = generateRequestId();
+  const timer = startTimer('api.compare-models');
+  const log = createLogger({ requestId, route: 'api.compare-models' });
+  
+  log.info('POST /api/compare-models - Processing comparison request');
+  
+  try {
+    // 1. Parse and validate request
+    const body = await req.json();
+    const { prompt, model1Id, model2Id, model1Name, model2Name } = body;
+    
+    if (!prompt || !model1Id || !model2Id) {
+      log.warn('Missing required fields', { 
+        hasPrompt: !!prompt, 
+        hasModel1: !!model1Id, 
+        hasModel2: !!model2Id 
+      });
+      return new Response('Missing required fields', { status: 400 });
+    }
+    
+    log.debug('Request parsed', {
+      model1Id,
+      model2Id,
+      model1Name,
+      model2Name,
+      promptLength: prompt.length
+    });
+    
+    // 2. Authenticate user
+    const session = await getServerSession();
+    if (!session) {
+      log.warn('Unauthorized request - no session');
+      timer({ status: 'error', reason: 'unauthorized' });
+      return new Response('Unauthorized', { status: 401 });
+    }
+    
+    // 3. Get current user
+    const currentUser = await getCurrentUserAction();
+    if (!currentUser.isSuccess) {
+      log.error('Failed to get current user');
+      return new Response('Unauthorized', { status: 401 });
+    }
+    
+    // 4. Get model configurations
+    const [model1Config, model2Config] = await Promise.all([
+      getModelConfig(model1Id),
+      getModelConfig(model2Id)
+    ]);
+    
+    if (!model1Config || !model2Config) {
+      log.error('One or both models not found', { 
+        model1Found: !!model1Config, 
+        model2Found: !!model2Config 
+      });
+      return new Response(
+        JSON.stringify({ error: 'One or both models not found' }),
+        { status: 404, headers: { 'Content-Type': 'application/json' } }
+      );
+    }
+    
+    log.info('Models configured', {
+      model1: { provider: model1Config.provider, modelId: model1Config.model_id },
+      model2: { provider: model2Config.provider, modelId: model2Config.model_id }
+    });
+    
+    // 5. Create messages for both models
+    const messages: UIMessage[] = [
+      {
+        id: generateRequestId(),
+        role: 'user',
+        parts: [{ type: 'text', text: prompt }]
+      }
+    ];
+    
+    // 6. Create SSE response stream
+    const encoder = new TextEncoder();
+    const stream = new ReadableStream({
+      async start(controller) {
+        try {
+          // Helper to send SSE data
+          const sendData = (data: Record<string, unknown>) => {
+            controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
+          };
+          
+          // Track completion status
+          let model1Complete = false;
+          let model2Complete = false;
+          
+          // Create stream requests for both models
+          const streamRequest1: StreamRequest = {
+            messages,
+            modelId: model1Config.model_id,
+            provider: model1Config.provider,
+            userId: currentUser.data.user.id.toString(),
+            sessionId: session.sub,
+            source: 'compare',
+            systemPrompt: `You are a helpful AI assistant. Please provide a clear and concise response.`,
+            callbacks: {
+              onProgress: (event) => {
+                // Stream model1 chunks
+                if (event.type === 'token' && event.text) {
+                  sendData({ model1: event.text });
+                }
+              },
+              onFinish: async ({ text, usage }) => {
+                log.info('Model 1 completed', {
+                  modelId: model1Config.model_id,
+                  tokensUsed: usage?.totalTokens
+                });
+                sendData({ model1Finished: true });
+                model1Complete = true;
+                
+                // Check if both models are complete
+                if (model1Complete && model2Complete) {
+                  sendData({ done: true });
+                  controller.close();
+                }
+              },
+              onError: (error) => {
+                log.error('Model 1 error', { error: error.message });
+                sendData({ model1Error: error.message });
+                model1Complete = true;
+                
+                if (model1Complete && model2Complete) {
+                  sendData({ done: true });
+                  controller.close();
+                }
+              }
+            }
+          };
+          
+          const streamRequest2: StreamRequest = {
+            messages,
+            modelId: model2Config.model_id,
+            provider: model2Config.provider,
+            userId: currentUser.data.user.id.toString(),
+            sessionId: session.sub,
+            source: 'compare',
+            systemPrompt: `You are a helpful AI assistant. Please provide a clear and concise response.`,
+            callbacks: {
+              onProgress: (event) => {
+                // Stream model2 chunks
+                if (event.type === 'token' && event.text) {
+                  sendData({ model2: event.text });
+                }
+              },
+              onFinish: async ({ text, usage }) => {
+                log.info('Model 2 completed', {
+                  modelId: model2Config.model_id,
+                  tokensUsed: usage?.totalTokens
+                });
+                sendData({ model2Finished: true });
+                model2Complete = true;
+                
+                // Check if both models are complete
+                if (model1Complete && model2Complete) {
+                  sendData({ done: true });
+                  controller.close();
+                }
+              },
+              onError: (error) => {
+                log.error('Model 2 error', { error: error.message });
+                sendData({ model2Error: error.message });
+                model2Complete = true;
+                
+                if (model1Complete && model2Complete) {
+                  sendData({ done: true });
+                  controller.close();
+                }
+              }
+            }
+          };
+          
+          // 7. Execute both streams in parallel using unified service
+          log.info('Starting parallel streams');
+          
+          await Promise.all([
+            unifiedStreamingService.stream(streamRequest1).catch(error => {
+              log.error('Failed to stream model 1', { error });
+              sendData({ model1Error: 'Failed to stream response' });
+              model1Complete = true;
+            }),
+            unifiedStreamingService.stream(streamRequest2).catch(error => {
+              log.error('Failed to stream model 2', { error });
+              sendData({ model2Error: 'Failed to stream response' });
+              model2Complete = true;
+            })
+          ]);
+          
+          timer({ status: 'success' });
+          
+        } catch (error) {
+          log.error('Stream error', { error });
+          controller.error(error);
+          timer({ status: 'error' });
+        }
+      }
+    });
+    
+    // Return SSE response
+    return new Response(stream, {
+      headers: {
+        'Content-Type': 'text/event-stream',
+        'Cache-Control': 'no-cache',
+        'Connection': 'keep-alive',
+        'X-Request-Id': requestId,
+        'X-Unified-Streaming': 'true'
+      }
+    });
+    
+  } catch (error) {
+    log.error('Compare API error', { 
+      error: error instanceof Error ? {
+        message: error.message,
+        name: error.name,
+        stack: error.stack
+      } : String(error)
+    });
+    
+    timer({ status: 'error' });
+    
+    return new Response(
+      JSON.stringify({
+        error: 'Failed to process comparison request',
+        details: error instanceof Error ? error.message : 'Unknown error',
+        requestId
+      }),
+      {
+        status: 500,
+        headers: {
+          'Content-Type': 'application/json',
+          'X-Request-Id': requestId
+        }
+      }
+    );
+  }
+}
\ No newline at end of file
diff --git a/app/api/conversations/route.ts b/app/api/conversations/route.ts
index 737e0de1..6f28b1cf 100644
--- a/app/api/conversations/route.ts
+++ b/app/api/conversations/route.ts
@@ -4,7 +4,7 @@ import { getCurrentUserAction } from '@/actions/db/get-current-user-action';
 import { executeSQL } from '@/lib/db/data-api-adapter';
 import { createLogger, generateRequestId, startTimer } from '@/lib/logger';
 
-export async function GET() {
+export async function GET(request: Request) {
   const requestId = generateRequestId();
   const timer = startTimer("api.conversations");
   const log = createLogger({ requestId, route: "api.conversations" });
@@ -30,7 +30,12 @@ export async function GET() {
   const userId = currentUser.data.user.id;
 
   return withErrorHandling(async () => {
-    const query = `
+    // Parse URL parameters
+    const url = new URL(request.url);
+    const latest = url.searchParams.get('latest') === 'true';
+    const limit = latest ? 1 : undefined;
+    
+    let query = `
       SELECT c.id, c.user_id, c.title, c.created_at, c.updated_at,
              c.model_id, c.source, c.execution_id, c.context,
              am.name as model_name, am.provider as model_provider,
@@ -42,6 +47,10 @@ export async function GET() {
       ORDER BY c.updated_at DESC
     `;
     
+    if (limit) {
+      query += ` LIMIT ${limit}`;
+    }
+    
     const parameters = [
       { name: 'userId', value: { longValue: userId } },
       { name: 'source', value: { stringValue: 'chat' } }
@@ -49,7 +58,11 @@ export async function GET() {
     
     const userConversations = await executeSQL(query, parameters);
     
-    log.info("Conversations retrieved successfully", { count: userConversations.length });
+    log.info("Conversations retrieved successfully", { 
+      count: userConversations.length,
+      latest,
+      limit 
+    });
     timer({ status: "success", count: userConversations.length });
     
     return userConversations;
diff --git a/app/api/models/capabilities/route.ts b/app/api/models/capabilities/route.ts
new file mode 100644
index 00000000..9dd5f62c
--- /dev/null
+++ b/app/api/models/capabilities/route.ts
@@ -0,0 +1,87 @@
+import { getModelCapabilities } from '@/app/api/chat/lib/provider-factory';
+import { createLogger, generateRequestId } from '@/lib/logger';
+import { ErrorFactories } from '@/lib/error-utils';
+
+export const runtime = 'nodejs';
+
+/**
+ * API endpoint to get model capabilities
+ * Used by frontend to understand model features and configure UI accordingly
+ */
+export async function POST(req: Request) {
+  const requestId = generateRequestId();
+  const log = createLogger({ requestId, route: 'api.models.capabilities' });
+  
+  log.info('POST /api/models/capabilities - Getting model capabilities');
+  
+  try {
+    const body = await req.json();
+    
+    // Validate required fields
+    if (!body.provider) {
+      throw ErrorFactories.validationFailed([
+        { field: 'provider', message: 'Provider is required' }
+      ]);
+    }
+    
+    if (!body.modelId) {
+      throw ErrorFactories.validationFailed([
+        { field: 'modelId', message: 'Model ID is required' }
+      ]);
+    }
+    
+    log.debug('Getting capabilities', {
+      provider: body.provider,
+      modelId: body.modelId
+    });
+    
+    // Get model capabilities
+    const capabilities = await getModelCapabilities(body.provider, body.modelId);
+    
+    log.info('Capabilities retrieved successfully', {
+      provider: body.provider,
+      modelId: body.modelId,
+      supportsReasoning: capabilities.supportsReasoning,
+      supportsThinking: capabilities.supportsThinking,
+      maxTimeoutMs: capabilities.maxTimeoutMs
+    });
+    
+    return Response.json(capabilities, {
+      headers: {
+        'X-Request-Id': requestId,
+        'Cache-Control': 'public, max-age=3600' // Cache for 1 hour
+      }
+    });
+    
+  } catch (error) {
+    log.error('Failed to get model capabilities', {
+      error: error instanceof Error ? error.message : String(error)
+    });
+    
+    if (error instanceof Error && error.message.includes('validation')) {
+      return Response.json(
+        {
+          error: 'Validation Error',
+          message: error.message,
+          requestId
+        },
+        {
+          status: 400,
+          headers: { 'X-Request-Id': requestId }
+        }
+      );
+    }
+    
+    return Response.json(
+      {
+        error: 'Internal Server Error',
+        message: 'Failed to get model capabilities',
+        requestId
+      },
+      {
+        status: 500,
+        headers: { 'X-Request-Id': requestId }
+      }
+    );
+  }
+}
\ No newline at end of file
diff --git a/lib/ai-helpers.ts b/lib/ai-helpers.ts
index 649121d0..48cd7047 100644
--- a/lib/ai-helpers.ts
+++ b/lib/ai-helpers.ts
@@ -1,6 +1,5 @@
 import { 
   generateText, 
-  streamText, 
   generateObject,
   embed,
   embedMany,
@@ -10,13 +9,16 @@ import {
   InvalidToolInputError,
   NoSuchToolError,
   ToolCallRepairError,
-  LanguageModel
+  LanguageModel,
+  UIMessage
 } from 'ai'
 import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock'
 import { createOpenAI } from '@ai-sdk/openai'
 import { z } from 'zod'
 import logger from "@/lib/logger"
 import { createProviderModel } from '@/app/api/chat/lib/provider-factory'
+import { unifiedStreamingService } from '@/lib/streaming/unified-streaming-service'
+import type { StreamRequest } from '@/lib/streaming/types'
 
 interface ModelConfig {
   provider: string
@@ -96,7 +98,7 @@ export async function generateCompletion(
   }
 }
 
-// Stream a text completion
+// Stream a text completion using unified streaming service
 export async function streamCompletion(
   modelConfig: ModelConfig,
   messages: CoreMessage[],
@@ -105,25 +107,80 @@ export async function streamCompletion(
   tools?: any
   // eslint-disable-next-line @typescript-eslint/no-explicit-any
 ): Promise<StreamTextResult<any, any>> {
-  const model = await getModelClient(modelConfig);
+  logger.info('[streamCompletion] Using unified streaming service', {
+    provider: modelConfig.provider,
+    modelId: modelConfig.modelId,
+    hasTools: !!tools
+  });
   
   try {
-    const result = await streamText({
-      model,
-      messages,
-      tools,
-      maxRetries: tools ? 5 : undefined,
-      onChunk: ({ chunk }) => {
-        if (chunk.type === 'text-delta' && options?.onToken) {
-          options.onToken((chunk as { text?: string }).text || '');
+    // Convert CoreMessage to UIMessage format for unified streaming
+    const uiMessages: UIMessage[] = messages.map((msg, index) => ({
+      id: `msg-${index}`,
+      role: msg.role as 'user' | 'assistant' | 'system',
+      parts: [{
+        type: 'text',
+        text: typeof msg.content === 'string' 
+          ? msg.content 
+          : JSON.stringify(msg.content)
+      }]
+    }));
+    
+    // Create stream request for unified service
+    const streamRequest: StreamRequest = {
+      messages: uiMessages,
+      modelId: modelConfig.modelId,
+      provider: modelConfig.provider,
+      source: 'ai-helpers', // Identify the source
+      userId: 'system', // For system-level calls
+      sessionId: 'system',
+      callbacks: {
+        onProgress: (event) => {
+          if (event.type === 'token' && options?.onToken && event.text) {
+            options.onToken(event.text);
+          }
+        },
+        onFinish: async (data) => {
+          if (options?.onFinish) {
+            options.onFinish(data);
+          }
+        },
+        onError: (error) => {
+          if (options?.onError) {
+            options.onError(error);
+          }
         }
-      },
-      onFinish: options?.onFinish
-    });
-
-    return result;
+      }
+    };
+    
+    // Note: The unified streaming service doesn't directly support tools yet
+    // For now, we'll fall back to the provider's native implementation when tools are needed
+    if (tools) {
+      logger.warn('[streamCompletion] Tools not yet supported in unified streaming, using fallback');
+      const model = await getModelClient(modelConfig);
+      const { streamText } = await import('ai');
+      return streamText({
+        model,
+        messages,
+        tools,
+        maxRetries: 5,
+        onChunk: ({ chunk }) => {
+          if (chunk.type === 'text-delta' && options?.onToken) {
+            options.onToken((chunk as { text?: string }).text || '');
+          }
+        },
+        onFinish: options?.onFinish
+      });
+    }
+    
+    // Use unified streaming service for non-tool calls
+    const response = await unifiedStreamingService.stream(streamRequest);
+    // The response.result is already a StreamTextResult-compatible object
+    // eslint-disable-next-line @typescript-eslint/no-explicit-any
+    return response.result as unknown as StreamTextResult<any, any>;
+    
   } catch (error) {
-    logger.error('[streamCompletion] Error during streaming:', error);
+    logger.error('[streamCompletion] Error during unified streaming:', error);
     throw error;
   }
 }
diff --git a/lib/error-utils.ts b/lib/error-utils.ts
index 29619412..e9a0cdf0 100644
--- a/lib/error-utils.ts
+++ b/lib/error-utils.ts
@@ -284,6 +284,14 @@ export const ErrorFactories = {
       `Quota exceeded for ${operation}. Limit: ${limit}, Current: ${current}`,
       { operation, quota: { limit, current, resetAt }, ...details }
     ),
+
+  // Streaming and Provider Errors
+  providerUnavailable: (provider: string, details?: Partial<ExternalServiceError>) =>
+    createTypedError<ExternalServiceError>(
+      ErrorCode.EXTERNAL_SERVICE_ERROR,
+      `Provider ${provider} is currently unavailable`,
+      { serviceName: provider, ...details }
+    ),
 }
 
 /**
diff --git a/lib/hooks/use-unified-stream.ts b/lib/hooks/use-unified-stream.ts
new file mode 100644
index 00000000..04f7cd7f
--- /dev/null
+++ b/lib/hooks/use-unified-stream.ts
@@ -0,0 +1,276 @@
+"use client";
+
+import { useChat } from '@ai-sdk/react';
+import { useState, useEffect, useCallback } from 'react';
+import { useToast } from '@/components/ui/use-toast';
+import { createLogger } from '@/lib/logger';
+import type { 
+  UseUnifiedStreamConfig, 
+  UseUnifiedStreamReturn,
+  ProviderCapabilities,
+  StreamRequest
+} from '@/lib/streaming/types';
+
+const log = createLogger({ module: 'use-unified-stream' });
+
+/**
+ * Unified streaming hook that provides a consistent interface
+ * for all AI streaming operations across the application
+ * 
+ * Features:
+ * - Automatic provider detection and capabilities
+ * - Reasoning and thinking content extraction
+ * - Adaptive timeouts based on model capabilities
+ * - Comprehensive error handling
+ * - Progress tracking and status updates
+ */
+export function useUnifiedStream(config: UseUnifiedStreamConfig): UseUnifiedStreamReturn {
+  const { toast } = useToast();
+  const [reasoning, setReasoning] = useState<string | null>(null);
+  const [thinking, setThinking] = useState<string | null>(null);
+  const [capabilities, setCapabilities] = useState<ProviderCapabilities | null>(null);
+  
+  // Use AI SDK's useChat with unified streaming endpoint
+  const {
+    messages,
+    setMessages,
+    status,
+    error: chatError,
+    sendMessage: baseSendMessage,
+    stop
+  } = useChat({
+    onFinish: (message) => {
+      log.debug('Stream finished', {
+        source: config.source,
+        messageLength: 0
+      });
+      
+      // Extract reasoning content if available
+      if ('reasoning' in message && typeof message.reasoning === 'string') {
+        setReasoning(message.reasoning);
+      }
+      
+      // Extract thinking content if available
+      if ('thinking' in message && typeof message.thinking === 'string') {
+        setThinking(message.thinking);
+      }
+      
+      toast({
+        title: 'Response Complete',
+        description: 'AI response generated successfully'
+      });
+    },
+    onError: (error) => {
+      log.error('Stream error', {
+        source: config.source,
+        error: error.message
+      });
+      
+      // Show user-friendly error messages
+      let errorTitle = 'AI Error';
+      let errorDescription = error.message;
+      
+      if (error.message.includes('timeout')) {
+        errorTitle = 'Request Timeout';
+        errorDescription = 'The AI model took too long to respond. Please try again.';
+      } else if (error.message.includes('quota') || error.message.includes('rate limit')) {
+        errorTitle = 'Rate Limit Exceeded';
+        errorDescription = 'Too many requests. Please wait a moment before trying again.';
+      } else if (error.message.includes('content')) {
+        errorTitle = 'Content Policy';
+        errorDescription = 'The request was blocked by content policy filters.';
+      }
+      
+      toast({
+        title: errorTitle,
+        description: errorDescription,
+        variant: 'destructive'
+      });
+    }
+  });
+  
+  /**
+   * Enhanced send message function with unified streaming support
+   */
+  const sendMessage = useCallback(async (
+    message: Parameters<typeof baseSendMessage>[0],
+    requestConfig?: Partial<StreamRequest>
+  ) => {
+    try {
+      log.debug('Sending message via unified stream', {
+        source: config.source,
+        modelId: config.modelId,
+        provider: config.provider,
+        hasConfig: !!requestConfig
+      });
+      
+      // Clear previous reasoning/thinking content
+      setReasoning(null);
+      setThinking(null);
+      
+      // Build request body with unified streaming configuration
+      const body = {
+        // Core configuration
+        source: config.source,
+        modelId: config.modelId || requestConfig?.modelId,
+        provider: config.provider || requestConfig?.provider,
+        
+        // System prompt and model configuration
+        systemPrompt: config.systemPrompt || requestConfig?.systemPrompt,
+        maxTokens: requestConfig?.maxTokens,
+        temperature: requestConfig?.temperature,
+        timeout: requestConfig?.timeout,
+        
+        // Advanced model options
+        reasoningEffort: config.options?.reasoningEffort || 'medium',
+        responseMode: config.options?.responseMode || 'standard',
+        backgroundMode: config.options?.backgroundMode || false,
+        thinkingBudget: config.options?.thinkingBudget,
+        enableWebSearch: config.options?.enableWebSearch || false,
+        enableCodeInterpreter: config.options?.enableCodeInterpreter || false,
+        enableImageGeneration: config.options?.enableImageGeneration || false,
+        
+        // Context from request config
+        conversationId: requestConfig?.conversationId,
+        executionId: requestConfig?.executionId,
+        documentId: requestConfig?.documentId,
+        
+        // Telemetry configuration
+        recordInputs: config.telemetry?.recordInputs,
+        recordOutputs: config.telemetry?.recordOutputs,
+        
+        // Add any additional fields from request config
+        ...requestConfig
+      };
+      
+      // Validate required fields
+      if (!body.modelId) {
+        throw new Error('Model ID is required for unified streaming');
+      }
+      
+      if (!body.provider) {
+        throw new Error('Provider is required for unified streaming');
+      }
+      
+      await baseSendMessage(message, { body });
+      
+    } catch (error) {
+      log.error('Failed to send message', {
+        source: config.source,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }, [config, baseSendMessage]);
+  
+  /**
+   * Clear all messages and state
+   */
+  const clear = useCallback(() => {
+    setMessages([]);
+    setReasoning(null);
+    setThinking(null);
+    setCapabilities(null);
+  }, [setMessages]);
+  
+  /**
+   * Fetch model capabilities when config changes
+   */
+  useEffect(() => {
+    async function fetchCapabilities() {
+      if (!config.modelId || !config.provider) {
+        return;
+      }
+      
+      try {
+        const response = await fetch('/api/models/capabilities', {
+          method: 'POST',
+          headers: { 'Content-Type': 'application/json' },
+          body: JSON.stringify({
+            provider: config.provider,
+            modelId: config.modelId
+          })
+        });
+        
+        if (response.ok) {
+          const caps = await response.json();
+          setCapabilities(caps);
+          
+          log.debug('Model capabilities loaded', {
+            modelId: config.modelId,
+            provider: config.provider,
+            supportsReasoning: caps.supportsReasoning,
+            supportsThinking: caps.supportsThinking
+          });
+        }
+      } catch (error) {
+        log.warn('Failed to fetch model capabilities', {
+          modelId: config.modelId,
+          provider: config.provider,
+          error: error instanceof Error ? error.message : String(error)
+        });
+      }
+    }
+    
+    fetchCapabilities();
+  }, [config.modelId, config.provider]);
+  
+  return {
+    messages,
+    status: status as UseUnifiedStreamReturn['status'],
+    error: chatError || null,
+    reasoning,
+    thinking,
+    sendMessage,
+    stop,
+    clear,
+    capabilities
+  };
+}
+
+/**
+ * Hook for chat-specific streaming with sensible defaults
+ */
+export function useChatStream(config?: Partial<UseUnifiedStreamConfig>) {
+  return useUnifiedStream({
+    source: 'chat',
+    ...config
+  });
+}
+
+/**
+ * Hook for model comparison streaming
+ */
+export function useCompareStream(config: {
+  model1?: { provider: string; modelId: string };
+  model2?: { provider: string; modelId: string };
+} & Partial<UseUnifiedStreamConfig>) {
+  // For comparison, we'll manage two separate streams
+  const stream1 = useUnifiedStream({
+    source: 'compare',
+    provider: config.model1?.provider,
+    modelId: config.model1?.modelId,
+    ...config
+  });
+  
+  const stream2 = useUnifiedStream({
+    source: 'compare',
+    provider: config.model2?.provider,
+    modelId: config.model2?.modelId,
+    ...config
+  });
+  
+  return { stream1, stream2 };
+}
+
+/**
+ * Hook for assistant execution streaming
+ */
+export function useAssistantStream(config: {
+  executionId?: number;
+} & Partial<UseUnifiedStreamConfig>) {
+  return useUnifiedStream({
+    source: 'assistant_execution',
+    ...config
+  });
+}
\ No newline at end of file
diff --git a/lib/streaming/__tests__/circuit-breaker.test.ts b/lib/streaming/__tests__/circuit-breaker.test.ts
new file mode 100644
index 00000000..b2580466
--- /dev/null
+++ b/lib/streaming/__tests__/circuit-breaker.test.ts
@@ -0,0 +1,209 @@
+import { describe, it, expect, beforeEach, afterEach, jest } from '@jest/globals';
+import { CircuitBreaker, CircuitBreakerState, CircuitBreakerOpenError } from '../circuit-breaker';
+
+// Mock logger
+jest.mock('@/lib/logger', () => ({
+  createLogger: () => ({
+    debug: jest.fn(),
+    info: jest.fn(),
+    warn: jest.fn(),
+    error: jest.fn()
+  })
+}));
+
+describe('CircuitBreaker', () => {
+  let circuitBreaker: CircuitBreaker;
+  
+  beforeEach(() => {
+    jest.useFakeTimers();
+    circuitBreaker = new CircuitBreaker({
+      failureThreshold: 3,
+      recoveryTimeoutMs: 5000,
+      monitoringPeriodMs: 10000,
+      successThreshold: 2
+    });
+  });
+
+  afterEach(() => {
+    jest.useRealTimers();
+  });
+
+  describe('initial state', () => {
+    it('should start in closed state', () => {
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.CLOSED);
+      expect(circuitBreaker.isOpen()).toBe(true); // Allows requests through
+    });
+
+    it('should have zero failures initially', () => {
+      const metrics = circuitBreaker.getMetrics();
+      expect(metrics.failureCount).toBe(0);
+      expect(metrics.successCount).toBe(0);
+      expect(metrics.recentFailures).toBe(0);
+    });
+  });
+
+  describe('failure handling', () => {
+    it('should remain closed with failures below threshold', () => {
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.CLOSED);
+      expect(circuitBreaker.isOpen()).toBe(true);
+    });
+
+    it('should open when failure threshold is reached', () => {
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure(); // Should trigger open state
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.OPEN);
+      expect(circuitBreaker.isOpen()).toBe(false); // Blocks requests
+    });
+
+    it('should track failure metrics correctly', () => {
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      
+      const metrics = circuitBreaker.getMetrics();
+      expect(metrics.failureCount).toBe(2);
+      expect(metrics.recentFailures).toBe(2);
+      expect(metrics.lastFailureTime).toBeGreaterThan(0);
+    });
+  });
+
+  describe('recovery handling', () => {
+    beforeEach(() => {
+      // Open the circuit breaker
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.OPEN);
+    });
+
+    it('should transition to half-open after recovery timeout', () => {
+      // Advance time past recovery timeout
+      jest.advanceTimersByTime(6000);
+      
+      // Check state should now be half-open
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.HALF_OPEN);
+      expect(circuitBreaker.isOpen()).toBe(true); // Allows limited requests
+    });
+
+    it('should close circuit after successful recovery', () => {
+      // Advance time past recovery timeout
+      jest.advanceTimersByTime(6000);
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.HALF_OPEN);
+      
+      // Record successful operations
+      circuitBreaker.recordSuccess();
+      circuitBreaker.recordSuccess(); // Should close the circuit
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.CLOSED);
+      expect(circuitBreaker.isOpen()).toBe(true);
+    });
+
+    it('should reopen circuit if failure occurs in half-open state', () => {
+      // Advance time past recovery timeout
+      jest.advanceTimersByTime(6000);
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.HALF_OPEN);
+      
+      // Record a failure in half-open state
+      circuitBreaker.recordFailure();
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.OPEN);
+      expect(circuitBreaker.isOpen()).toBe(false);
+    });
+  });
+
+  describe('monitoring window', () => {
+    it('should clean old failures outside monitoring window', () => {
+      // Record failures
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      
+      expect(circuitBreaker.getMetrics().recentFailures).toBe(2);
+      
+      // Advance time past monitoring period
+      jest.advanceTimersByTime(11000);
+      
+      // Check that old failures are cleaned up
+      const metrics = circuitBreaker.getMetrics();
+      expect(metrics.recentFailures).toBe(0);
+    });
+
+    it('should reset failure count on success in closed state', () => {
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      
+      expect(circuitBreaker.getMetrics().failureCount).toBe(2);
+      
+      circuitBreaker.recordSuccess();
+      
+      expect(circuitBreaker.getMetrics().failureCount).toBe(0);
+    });
+  });
+
+  describe('manual reset', () => {
+    it('should reset to closed state when manually reset', () => {
+      // Open the circuit
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      circuitBreaker.recordFailure();
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.OPEN);
+      
+      // Manual reset
+      circuitBreaker.reset();
+      
+      expect(circuitBreaker.getState()).toBe(CircuitBreakerState.CLOSED);
+      expect(circuitBreaker.isOpen()).toBe(true);
+      
+      const metrics = circuitBreaker.getMetrics();
+      expect(metrics.failureCount).toBe(0);
+      expect(metrics.successCount).toBe(0);
+      expect(metrics.recentFailures).toBe(0);
+    });
+  });
+
+  describe('configuration edge cases', () => {
+    it('should handle zero failure threshold', () => {
+      const cb = new CircuitBreaker({
+        failureThreshold: 0,
+        recoveryTimeoutMs: 1000,
+        monitoringPeriodMs: 5000
+      });
+      
+      expect(cb.getState()).toBe(CircuitBreakerState.CLOSED);
+      
+      cb.recordFailure();
+      
+      // Should open immediately with zero threshold
+      expect(cb.getState()).toBe(CircuitBreakerState.OPEN);
+    });
+
+    it('should handle very short recovery timeout', () => {
+      const cb = new CircuitBreaker({
+        failureThreshold: 1,
+        recoveryTimeoutMs: 10, // Very short
+        monitoringPeriodMs: 5000
+      });
+      
+      cb.recordFailure();
+      expect(cb.getState()).toBe(CircuitBreakerState.OPEN);
+      
+      // Advance time briefly
+      jest.advanceTimersByTime(20);
+      
+      expect(cb.getState()).toBe(CircuitBreakerState.HALF_OPEN);
+    });
+  });
+});
+
+describe('CircuitBreakerOpenError', () => {
+  it('should create error with provider and state', () => {
+    const error = new CircuitBreakerOpenError('openai', CircuitBreakerState.OPEN);
+    
+    expect(error.name).toBe('CircuitBreakerOpenError');
+    expect(error.message).toBe('Circuit breaker is open for provider: openai');
+  });
+});
\ No newline at end of file
diff --git a/lib/streaming/__tests__/openai-responses-api.test.ts b/lib/streaming/__tests__/openai-responses-api.test.ts
new file mode 100644
index 00000000..83185ae2
--- /dev/null
+++ b/lib/streaming/__tests__/openai-responses-api.test.ts
@@ -0,0 +1,314 @@
+import { describe, it, expect, beforeEach, jest } from '@jest/globals';
+import { 
+  createResponsesAPIClient, 
+  ResponsesAPIClient,
+  streamWithResponsesAPI 
+} from '../openai-responses-api';
+import type { StreamRequest, StreamingCallbacks } from '../types';
+
+// Mock fetch globally
+global.fetch = jest.fn() as jest.MockedFunction<typeof fetch>;
+
+// Polyfill ReadableStream for Node.js test environment
+if (!global.ReadableStream) {
+  global.ReadableStream = class ReadableStream {
+    private controller: {
+      enqueue: jest.MockedFunction<(chunk: unknown) => void>;
+      close: jest.MockedFunction<() => void>;
+    };
+    
+    constructor(source: { start?: (controller: unknown) => void }) {
+      this.controller = {
+        enqueue: jest.fn(),
+        close: jest.fn()
+      };
+      if (source?.start) {
+        source.start(this.controller);
+      }
+    }
+    
+    getReader() {
+      const controller = this.controller;
+      let chunks: unknown[] = [];
+      let currentIndex = 0;
+      
+      // Extract chunks from controller.enqueue calls
+      if (controller.enqueue.mock) {
+        chunks = controller.enqueue.mock.calls.map((call: unknown[]) => call[0]);
+      }
+      
+      return {
+        read: async () => {
+          if (currentIndex < chunks.length) {
+            return { done: false, value: chunks[currentIndex++] };
+          }
+          return { done: true, value: undefined };
+        },
+        releaseLock: jest.fn()
+      };
+    }
+  } as unknown as typeof ReadableStream;
+  
+  global.TextEncoder = class TextEncoder {
+    encode(text: string): Uint8Array {
+      return Buffer.from(text, 'utf-8');
+    }
+  } as unknown as typeof TextEncoder;
+  
+  global.TextDecoder = class TextDecoder {
+    decode(buffer: BufferSource): string {
+      return Buffer.from(buffer as ArrayBuffer).toString('utf-8');
+    }
+  } as unknown as typeof TextDecoder;
+}
+
+// Mock logger
+jest.mock('@/lib/logger', () => ({
+  createLogger: () => ({
+    debug: jest.fn(),
+    info: jest.fn(),
+    warn: jest.fn(),
+    error: jest.fn()
+  })
+}));
+
+describe('OpenAI Responses API', () => {
+  beforeEach(() => {
+    jest.clearAllMocks();
+    (global.fetch as jest.MockedFunction<typeof fetch>).mockClear();
+  });
+  
+  describe('ResponsesAPIClient', () => {
+    it('should create client with config', () => {
+      const client = createResponsesAPIClient({
+        apiKey: 'test-key',
+        modelId: 'o3-mini',
+        reasoningEffort: 'high',
+        backgroundMode: false
+      });
+      
+      expect(client).toBeInstanceOf(ResponsesAPIClient);
+    });
+    
+    it('should handle streaming responses with reasoning', async () => {
+      // Mock SSE stream response
+      const mockStream = new ReadableStream({
+        start(controller) {
+          controller.enqueue(new TextEncoder().encode('data: {"type":"reasoning_step","content":"Step 1: Analyzing","step_number":1,"tokens":5}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: {"type":"response_delta","content":"The answer is "}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: {"type":"response_delta","content":"42"}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: {"type":"finish","usage":{"prompt_tokens":10,"completion_tokens":20,"total_tokens":30},"finish_reason":"stop"}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
+          controller.close();
+        }
+      });
+      
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: true,
+        status: 200,
+        statusText: 'OK',
+        body: mockStream
+      } as Response);
+      
+      const client = createResponsesAPIClient({
+        apiKey: 'test-key',
+        modelId: 'o3-mini',
+        reasoningEffort: 'medium'
+      });
+      
+      const callbacks: StreamingCallbacks = {
+        onReasoning: jest.fn(),
+        onProgress: jest.fn(),
+        onFinish: jest.fn()
+      };
+      
+      const result = await client.stream([
+        { role: 'user', content: 'What is the answer?' }
+      ], callbacks);
+      
+      expect(result.status).toBe('completed');
+      expect(result.reasoning).toEqual(['Step 1: Analyzing']);
+      expect(result.response).toBe('The answer is 42');
+      expect(result.reasoningTokens).toBe(5);
+      
+      expect(callbacks.onReasoning).toHaveBeenCalledWith('Step 1: Analyzing');
+      expect(callbacks.onProgress).toHaveBeenCalled();
+      expect(callbacks.onFinish).toHaveBeenCalledWith(expect.objectContaining({
+        text: 'The answer is 42',
+        usage: expect.objectContaining({
+          promptTokens: 10,
+          completionTokens: 20,
+          totalTokens: 30,
+          reasoningTokens: 5
+        })
+      }));
+    });
+    
+    it('should handle background mode', async () => {
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: true,
+        status: 200,
+        statusText: 'OK',
+        json: async () => ({
+          job_id: 'job-123',
+          estimated_completion_time: 30000
+        })
+      } as Response);
+      
+      const client = createResponsesAPIClient({
+        apiKey: 'test-key',
+        modelId: 'o4',
+        backgroundMode: true
+      });
+      
+      const result = await client.stream([
+        { role: 'user', content: 'Solve this complex problem' }
+      ]);
+      
+      expect(result.status).toBe('background');
+      expect(result.jobId).toBe('job-123');
+      
+      expect(global.fetch).toHaveBeenCalledWith(
+        'https://api.openai.com/v1/responses',
+        expect.objectContaining({
+          method: 'POST',
+          headers: expect.objectContaining({
+            'Authorization': 'Bearer test-key',
+            'OpenAI-Beta': 'responses-api-v1'
+          }),
+          body: expect.stringContaining('"background_mode":true')
+        })
+      );
+    });
+    
+    it('should poll for job status', async () => {
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: true,
+        status: 200,
+        json: async () => ({
+          status: 'completed',
+          reasoning_steps: ['Step 1', 'Step 2'],
+          thinking_time_ms: 5000,
+          reasoning_tokens: 150,
+          response: 'Complex solution'
+        })
+      } as Response);
+      
+      const client = createResponsesAPIClient({
+        apiKey: 'test-key',
+        modelId: 'o4'
+      });
+      
+      const result = await client.getJobStatus('job-123');
+      
+      expect(result.status).toBe('completed');
+      expect(result.reasoning).toEqual(['Step 1', 'Step 2']);
+      expect(result.thinkingTime).toBe(5000);
+      expect(result.reasoningTokens).toBe(150);
+      expect(result.response).toBe('Complex solution');
+    });
+    
+    it('should handle tool calls in reasoning', async () => {
+      const mockStream = new ReadableStream({
+        start(controller) {
+          controller.enqueue(new TextEncoder().encode('data: {"type":"tool_call","tool_name":"calculator","arguments":{"expression":"2+2"}}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: {"type":"response_delta","content":"Result: 4"}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
+          controller.close();
+        }
+      });
+      
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: true,
+        body: mockStream
+      } as Response);
+      
+      const client = createResponsesAPIClient({
+        apiKey: 'test-key',
+        modelId: 'gpt-5'
+      });
+      
+      const callbacks: StreamingCallbacks = {
+        onProgress: jest.fn()
+      };
+      
+      await client.stream([
+        { role: 'user', content: 'Calculate 2+2' }
+      ], callbacks);
+      
+      expect(callbacks.onProgress).toHaveBeenCalledWith(
+        expect.objectContaining({
+          type: 'tool_call',
+          metadata: expect.objectContaining({
+            tool_name: 'calculator',
+            arguments: { expression: '2+2' }
+          })
+        })
+      );
+    });
+    
+    it('should handle API errors', async () => {
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: false,
+        status: 401,
+        statusText: 'Unauthorized'
+      } as Response);
+      
+      const client = createResponsesAPIClient({
+        apiKey: 'invalid-key',
+        modelId: 'o3'
+      });
+      
+      await expect(client.stream([
+        { role: 'user', content: 'Test' }
+      ])).rejects.toThrow('Responses API error: 401 Unauthorized');
+    });
+  });
+  
+  describe('streamWithResponsesAPI', () => {
+    it('should integrate with unified streaming format', async () => {
+      const mockStream = new ReadableStream({
+        start(controller) {
+          controller.enqueue(new TextEncoder().encode('data: {"type":"response_delta","content":"Test response"}\n\n'));
+          controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
+          controller.close();
+        }
+      });
+      
+      (global.fetch as jest.MockedFunction<typeof fetch>).mockResolvedValueOnce({
+        ok: true,
+        body: mockStream
+      } as Response);
+      
+      // Mock env variable
+      process.env.OPENAI_API_KEY = 'test-key';
+      
+      const request: StreamRequest = {
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Test message' }]
+          }
+        ],
+        modelId: 'o3-mini',
+        provider: 'openai',
+        source: 'chat',
+        userId: 'test-user',
+        options: {
+          reasoningEffort: 'high',
+          backgroundMode: false
+        }
+      };
+      
+      const result = await streamWithResponsesAPI(request);
+      
+      expect(result.capabilities.supportsReasoning).toBe(true);
+      expect(result.capabilities.supportsBackgroundMode).toBe(true);
+      expect(result.result).toBeDefined();
+      
+      const usage = await result.result.usage;
+      expect(usage).toBeDefined();
+    });
+  });
+});
\ No newline at end of file
diff --git a/lib/streaming/__tests__/unified-streaming-service.test.ts b/lib/streaming/__tests__/unified-streaming-service.test.ts
new file mode 100644
index 00000000..b0a1c64d
--- /dev/null
+++ b/lib/streaming/__tests__/unified-streaming-service.test.ts
@@ -0,0 +1,348 @@
+/* eslint-disable @typescript-eslint/no-explicit-any */
+import { describe, it, expect, beforeEach, jest } from '@jest/globals';
+import { UnifiedStreamingService } from '../unified-streaming-service';
+import { getTelemetryConfig } from '../telemetry-service';
+import { getProviderAdapter } from '../provider-adapters';
+import type { StreamRequest, ProviderCapabilities, ProviderAdapter } from '../types';
+
+// Mock dependencies
+jest.mock('../telemetry-service');
+jest.mock('../provider-adapters');
+jest.mock('@/lib/logger', () => ({
+  createLogger: () => ({
+    debug: jest.fn(),
+    info: jest.fn(),
+    warn: jest.fn(),
+    error: jest.fn()
+  }),
+  generateRequestId: () => 'test-request-id',
+  startTimer: () => jest.fn()
+}));
+
+// Type assertions for mocked functions
+const mockGetTelemetryConfig = getTelemetryConfig as jest.MockedFunction<typeof getTelemetryConfig>;
+const mockGetProviderAdapter = getProviderAdapter as jest.MockedFunction<typeof getProviderAdapter>;
+
+describe('UnifiedStreamingService', () => {
+  let streamingService: UnifiedStreamingService;
+  let mockAdapter: {
+    createModel: jest.Mock;
+    getCapabilities: jest.Mock;
+    getProviderOptions: jest.Mock;
+    streamWithEnhancements: jest.Mock;
+  };
+  let mockTelemetryConfig: {
+    isEnabled: boolean;
+    functionId: string;
+    metadata: Record<string, unknown>;
+    recordInputs: boolean;
+    recordOutputs: boolean;
+    tracer: {
+      startSpan: jest.Mock;
+    };
+  };
+
+  beforeEach(() => {
+    streamingService = new UnifiedStreamingService();
+    
+    mockAdapter = {
+      createModel: jest.fn(),
+      getCapabilities: jest.fn(),
+      getProviderOptions: jest.fn(),
+      streamWithEnhancements: jest.fn()
+    };
+    
+    mockTelemetryConfig = {
+      isEnabled: true,
+      functionId: 'test-function',
+      metadata: {} as Record<string, string | number | boolean>,
+      recordInputs: true,
+      recordOutputs: true,
+      tracer: {
+        startSpan: jest.fn(() => ({
+          setAttributes: jest.fn(),
+          addEvent: jest.fn(),
+          recordException: jest.fn(),
+          setStatus: jest.fn(),
+          end: jest.fn()
+        }))
+      }
+    };
+    
+    (mockGetProviderAdapter as any).mockResolvedValue(mockAdapter as unknown as ProviderAdapter);
+    (mockGetTelemetryConfig as any).mockResolvedValue(mockTelemetryConfig);
+  });
+
+  describe('stream', () => {
+    it('should successfully stream with OpenAI provider', async () => {
+      // Arrange
+      const request: StreamRequest = {
+        provider: 'openai',
+        modelId: 'gpt-4',
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Hello' }]
+          }
+        ],
+        source: 'chat',
+        userId: 'test-user'
+      };
+
+      const mockCapabilities: ProviderCapabilities = {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 1000,
+        maxTimeoutMs: 30000,
+        costPerInputToken: 0.00001,
+        costPerOutputToken: 0.00003
+      };
+
+      const mockStreamResult = {
+        toDataStreamResponse: jest.fn(),
+        usage: Promise.resolve({
+          totalTokens: 100,
+          promptTokens: 50,
+          completionTokens: 50
+        })
+      };
+
+      (mockAdapter.getCapabilities as any).mockReturnValue(mockCapabilities);
+      (mockAdapter.createModel as any).mockResolvedValue('mock-model' as any);
+      (mockAdapter.getProviderOptions as any).mockReturnValue({});
+      (mockAdapter.streamWithEnhancements as any).mockResolvedValue(mockStreamResult as any);
+
+      // Act
+      const result = await streamingService.stream(request);
+
+      // Assert
+      expect(result).toBeDefined();
+      expect(result.result).toBe(mockStreamResult);
+      expect(result.capabilities).toBe(mockCapabilities);
+      expect(mockAdapter.streamWithEnhancements).toHaveBeenCalled();
+    });
+
+    it('should handle reasoning models with extended timeout', async () => {
+      // Arrange
+      const request: StreamRequest = {
+        provider: 'openai',
+        modelId: 'o3-mini',
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Solve this complex problem' }]
+          }
+        ],
+        source: 'chat',
+        userId: 'test-user',
+        options: {
+          reasoningEffort: 'high'
+        }
+      };
+
+      const mockCapabilities: ProviderCapabilities = {
+        supportsReasoning: true,
+        supportsThinking: false,
+        supportedResponseModes: ['standard', 'flex', 'priority'],
+        supportsBackgroundMode: true,
+        supportedTools: ['web_search', 'code_interpreter'],
+        typicalLatencyMs: 10000,
+        maxTimeoutMs: 600000,
+        costPerInputToken: 0.00015,
+        costPerOutputToken: 0.0006,
+        costPerReasoningToken: 0.0003
+      };
+
+      const mockStreamResult = {
+        toDataStreamResponse: jest.fn(),
+        usage: Promise.resolve({
+          totalTokens: 500,
+          promptTokens: 100,
+          completionTokens: 200,
+          reasoningTokens: 200
+        })
+      };
+
+      (mockAdapter.getCapabilities as any).mockReturnValue(mockCapabilities);
+      (mockAdapter.createModel as any).mockResolvedValue('mock-model' as any);
+      (mockAdapter.getProviderOptions as any).mockReturnValue({});
+      (mockAdapter.streamWithEnhancements as any).mockResolvedValue(mockStreamResult as any);
+
+      // Act
+      const result = await streamingService.stream(request);
+
+      // Assert
+      expect(result).toBeDefined();
+      expect(mockAdapter.streamWithEnhancements).toHaveBeenCalled();
+    });
+
+    it('should handle Claude thinking models', async () => {
+      // Arrange
+      const request: StreamRequest = {
+        provider: 'amazon-bedrock',
+        modelId: 'claude-4-opus',
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Think through this step by step' }]
+          }
+        ],
+        source: 'chat',
+        userId: 'test-user',
+        options: {
+          thinkingBudget: 4000
+        }
+      };
+
+      const mockCapabilities: ProviderCapabilities = {
+        supportsReasoning: false,
+        supportsThinking: true,
+        maxThinkingTokens: 6553,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 3000,
+        maxTimeoutMs: 120000,
+        costPerInputToken: 0.000015,
+        costPerOutputToken: 0.000075
+      };
+
+      const mockStreamResult = {
+        toDataStreamResponse: jest.fn(),
+        usage: Promise.resolve({
+          totalTokens: 300,
+          promptTokens: 100,
+          completionTokens: 150,
+          thinkingTokens: 50
+        })
+      };
+
+      (mockAdapter.getCapabilities as any).mockReturnValue(mockCapabilities);
+      (mockAdapter.createModel as any).mockResolvedValue('mock-model' as any);
+      (mockAdapter.getProviderOptions as any).mockReturnValue({
+        anthropic: {
+          thinkingBudget: 4000,
+          enableThinking: true,
+          streamThinking: true
+        }
+      });
+      (mockAdapter.streamWithEnhancements as any).mockResolvedValue(mockStreamResult as any);
+
+      // Act
+      const result = await streamingService.stream(request);
+
+      // Assert
+      expect(result).toBeDefined();
+      expect(mockAdapter.streamWithEnhancements).toHaveBeenCalled();
+    });
+
+    it('should handle circuit breaker open state', async () => {
+      // Arrange
+      const request: StreamRequest = {
+        provider: 'failing-provider',
+        modelId: 'test-model',
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Hello' }]
+          }
+        ],
+        source: 'chat',
+        userId: 'test-user'
+      };
+
+      // Create a failing adapter to trip the circuit breaker
+      const failingAdapter = {
+        ...mockAdapter,
+        streamWithEnhancements: jest.fn(() => Promise.reject(new Error('Provider failure')))
+      };
+
+      (mockGetProviderAdapter as any).mockResolvedValue(failingAdapter as unknown as ProviderAdapter);
+
+      // Trip the circuit breaker by failing multiple times
+      const streamingServiceWithFailures = new UnifiedStreamingService();
+      
+      // First, fail enough times to open the circuit
+      for (let i = 0; i < 5; i++) {
+        try {
+          await streamingServiceWithFailures.stream(request);
+        } catch {
+          // Expected to fail
+        }
+      }
+
+      // Act & Assert
+      await expect(streamingServiceWithFailures.stream(request)).rejects.toThrow();
+    });
+
+    it('should record telemetry correctly', async () => {
+      // Arrange
+      const request: StreamRequest = {
+        provider: 'openai',
+        modelId: 'gpt-4',
+        messages: [
+          {
+            id: '1',
+            role: 'user',
+            parts: [{ type: 'text', text: 'Hello' }]
+          }
+        ],
+        source: 'chat',
+        userId: 'test-user',
+        telemetry: {
+          recordInputs: true,
+          recordOutputs: true
+        }
+      };
+
+      const mockCapabilities: ProviderCapabilities = {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 1000,
+        maxTimeoutMs: 30000,
+        costPerInputToken: 0.00001,
+        costPerOutputToken: 0.00003
+      };
+
+      const mockStreamResult = {
+        toDataStreamResponse: jest.fn(),
+        usage: Promise.resolve({
+          totalTokens: 100,
+          promptTokens: 50,
+          completionTokens: 50
+        })
+      };
+
+      (mockAdapter.getCapabilities as any).mockReturnValue(mockCapabilities);
+      (mockAdapter.createModel as any).mockResolvedValue('mock-model' as any);
+      (mockAdapter.getProviderOptions as any).mockReturnValue({});
+      (mockAdapter.streamWithEnhancements as any).mockResolvedValue(mockStreamResult as any);
+
+      // Act
+      await streamingService.stream(request);
+
+      // Assert
+      expect(mockGetTelemetryConfig).toHaveBeenCalledWith({
+        functionId: 'chat.stream',
+        userId: 'test-user',
+        sessionId: undefined,
+        conversationId: undefined,
+        modelId: 'gpt-4',
+        provider: 'openai',
+        source: 'chat',
+        recordInputs: true,
+        recordOutputs: true
+      });
+    });
+  });
+});
\ No newline at end of file
diff --git a/lib/streaming/circuit-breaker.ts b/lib/streaming/circuit-breaker.ts
new file mode 100644
index 00000000..82d97e4a
--- /dev/null
+++ b/lib/streaming/circuit-breaker.ts
@@ -0,0 +1,216 @@
+import { createLogger } from '@/lib/logger';
+
+const log = createLogger({ module: 'circuit-breaker' });
+
+/**
+ * Circuit breaker states
+ */
+export enum CircuitBreakerState {
+  CLOSED = 'closed',     // Normal operation
+  OPEN = 'open',         // Failing fast
+  HALF_OPEN = 'half-open' // Testing if service recovered
+}
+
+/**
+ * Circuit breaker configuration
+ */
+export interface CircuitBreakerConfig {
+  failureThreshold: number;      // Number of failures before opening
+  recoveryTimeoutMs: number;     // Time to wait before attempting recovery
+  monitoringPeriodMs: number;    // Window for failure counting
+  successThreshold?: number;     // Successes needed to close from half-open
+}
+
+/**
+ * Circuit breaker for AI provider resilience
+ * Prevents cascade failures and provides fast failure when providers are down
+ */
+export class CircuitBreaker {
+  private state: CircuitBreakerState = CircuitBreakerState.CLOSED;
+  private failureCount = 0;
+  private successCount = 0;
+  private lastFailureTime = 0;
+  private lastSuccessTime = 0;
+  private failures: number[] = []; // Timestamps of failures
+  private readonly config: Required<CircuitBreakerConfig>;
+
+  constructor(config: CircuitBreakerConfig) {
+    this.config = {
+      successThreshold: 2,
+      ...config
+    };
+    
+    log.debug('Circuit breaker created', {
+      failureThreshold: this.config.failureThreshold,
+      recoveryTimeoutMs: this.config.recoveryTimeoutMs,
+      monitoringPeriodMs: this.config.monitoringPeriodMs,
+      successThreshold: this.config.successThreshold
+    });
+  }
+
+  /**
+   * Check if circuit breaker is open (blocking requests)
+   * @returns true if circuit is OPEN (requests blocked), false if CLOSED or HALF_OPEN (requests allowed)
+   */
+  isOpen(): boolean {
+    this.updateState();
+    return this.state === CircuitBreakerState.OPEN;
+  }
+
+  /**
+   * Check if circuit breaker allows requests through
+   * @returns true if requests are allowed (CLOSED or HALF_OPEN), false if blocked (OPEN)
+   */
+  allowsRequests(): boolean {
+    this.updateState();
+    return this.state !== CircuitBreakerState.OPEN;
+  }
+
+  /**
+   * Record a successful operation
+   */
+  recordSuccess(): void {
+    this.lastSuccessTime = Date.now();
+
+    if (this.state === CircuitBreakerState.HALF_OPEN) {
+      this.successCount++;
+      log.debug('Success recorded in half-open state', {
+        successCount: this.successCount,
+        threshold: this.config.successThreshold
+      });
+
+      if (this.successCount >= this.config.successThreshold) {
+        this.reset();
+        log.info('Circuit breaker closed after successful recovery');
+      }
+    } else if (this.state === CircuitBreakerState.CLOSED) {
+      // Clear old failures on success
+      this.cleanOldFailures();
+      if (this.failureCount > 0) {
+        log.debug('Resetting failure count after success');
+        this.failureCount = 0;
+      }
+    }
+  }
+
+  /**
+   * Record a failed operation
+   */
+  recordFailure(): void {
+    const now = Date.now();
+    this.lastFailureTime = now;
+    this.failures.push(now);
+    this.failureCount++;
+
+    log.debug('Failure recorded', {
+      failureCount: this.failureCount,
+      threshold: this.config.failureThreshold,
+      state: this.state
+    });
+
+    if (this.state === CircuitBreakerState.HALF_OPEN) {
+      // Any failure in half-open immediately opens the circuit
+      this.state = CircuitBreakerState.OPEN;
+      log.warn('Circuit breaker opened due to failure in half-open state');
+    } else if (this.state === CircuitBreakerState.CLOSED) {
+      this.cleanOldFailures();
+      const recentFailures = this.failures.length;
+      
+      if (recentFailures >= this.config.failureThreshold) {
+        this.state = CircuitBreakerState.OPEN;
+        log.error('Circuit breaker opened due to failure threshold exceeded', {
+          recentFailures,
+          threshold: this.config.failureThreshold
+        });
+      }
+    }
+  }
+
+  /**
+   * Get current circuit breaker state
+   */
+  getState(): CircuitBreakerState {
+    this.updateState();
+    return this.state;
+  }
+
+  /**
+   * Get circuit breaker metrics
+   */
+  getMetrics(): {
+    state: CircuitBreakerState;
+    failureCount: number;
+    successCount: number;
+    lastFailureTime: number;
+    lastSuccessTime: number;
+    recentFailures: number;
+  } {
+    this.cleanOldFailures();
+    return {
+      state: this.state,
+      failureCount: this.failureCount,
+      successCount: this.successCount,
+      lastFailureTime: this.lastFailureTime,
+      lastSuccessTime: this.lastSuccessTime,
+      recentFailures: this.failures.length
+    };
+  }
+
+  /**
+   * Manually reset the circuit breaker to closed state
+   */
+  reset(): void {
+    this.state = CircuitBreakerState.CLOSED;
+    this.failureCount = 0;
+    this.successCount = 0;
+    this.failures = [];
+    log.info('Circuit breaker manually reset to closed state');
+  }
+
+  /**
+   * Update state based on time and current conditions
+   */
+  private updateState(): void {
+    const now = Date.now();
+
+    if (this.state === CircuitBreakerState.OPEN) {
+      // Check if recovery timeout has elapsed
+      if (now - this.lastFailureTime >= this.config.recoveryTimeoutMs) {
+        this.state = CircuitBreakerState.HALF_OPEN;
+        this.successCount = 0;
+        log.info('Circuit breaker transitioned to half-open for recovery test');
+      }
+    }
+
+    // Clean old failures regardless of state
+    this.cleanOldFailures();
+  }
+
+  /**
+   * Remove failures outside the monitoring window
+   */
+  private cleanOldFailures(): void {
+    const now = Date.now();
+    const cutoff = now - this.config.monitoringPeriodMs;
+    const originalLength = this.failures.length;
+    
+    this.failures = this.failures.filter(timestamp => timestamp > cutoff);
+    
+    if (this.failures.length !== originalLength) {
+      log.debug('Cleaned old failures', {
+        removed: originalLength - this.failures.length,
+        remaining: this.failures.length
+      });
+    }
+  }
+}
+
+/**
+ * Circuit breaker error for when requests are blocked
+ */
+export class CircuitBreakerOpenError extends Error {
+  constructor(provider: string, state: CircuitBreakerState) {
+    super(`Circuit breaker is ${state} for provider: ${provider}`);
+    this.name = 'CircuitBreakerOpenError';
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/openai-responses-api.ts b/lib/streaming/openai-responses-api.ts
new file mode 100644
index 00000000..5a7d1a5a
--- /dev/null
+++ b/lib/streaming/openai-responses-api.ts
@@ -0,0 +1,380 @@
+import { createLogger } from '@/lib/logger';
+import type { StreamRequest, StreamResponse, StreamingCallbacks } from './types';
+
+const log = createLogger({ module: 'openai-responses-api' });
+
+/**
+ * OpenAI Responses API Implementation
+ * 
+ * The Responses API is designed for reasoning models (o3, o4, GPT-5) and provides:
+ * - Structured reasoning with step-by-step explanations
+ * - Background processing for long-running reasoning tasks
+ * - Reasoning effort control (minimal, low, medium, high)
+ * - Reasoning token tracking and cost optimization
+ * - Persistent reasoning context across conversations
+ */
+
+export interface ResponsesAPIConfig {
+  apiKey: string;
+  modelId: string;
+  reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high';
+  backgroundMode?: boolean;
+  streamReasoningSummaries?: boolean;
+  preserveReasoningItems?: boolean;
+  maxReasoningTokens?: number;
+}
+
+export interface ResponsesAPIResult {
+  jobId?: string;
+  status: 'streaming' | 'background' | 'completed' | 'failed';
+  reasoning?: string[];
+  thinkingTime?: number;
+  reasoningTokens?: number;
+  response: string;
+}
+
+/**
+ * Create a Responses API client for reasoning models
+ */
+export function createResponsesAPIClient(config: ResponsesAPIConfig) {
+  return new ResponsesAPIClient(config);
+}
+
+export class ResponsesAPIClient {
+  private apiKey: string;
+  private baseURL = 'https://api.openai.com/v1/responses';
+  
+  constructor(private config: ResponsesAPIConfig) {
+    this.apiKey = config.apiKey;
+  }
+  
+  /**
+   * Stream a response with reasoning
+   */
+  async stream(
+    messages: Array<{ role: string; content: string }>,
+    callbacks?: StreamingCallbacks
+  ): Promise<ResponsesAPIResult> {
+    const startTime = Date.now();
+    
+    log.info('Starting Responses API stream', {
+      model: this.config.modelId,
+      reasoningEffort: this.config.reasoningEffort,
+      backgroundMode: this.config.backgroundMode,
+      messageCount: messages.length
+    });
+    
+    try {
+      const response = await fetch(this.baseURL, {
+        method: 'POST',
+        headers: {
+          'Authorization': `Bearer ${this.apiKey}`,
+          'Content-Type': 'application/json',
+          'OpenAI-Beta': 'responses-api-v1'
+        },
+        body: JSON.stringify({
+          model: this.config.modelId,
+          messages,
+          reasoning_effort: this.config.reasoningEffort || 'medium',
+          background_mode: this.config.backgroundMode || false,
+          stream: !this.config.backgroundMode,
+          stream_reasoning_summaries: this.config.streamReasoningSummaries ?? true,
+          preserve_reasoning_items: this.config.preserveReasoningItems ?? true,
+          max_reasoning_tokens: this.config.maxReasoningTokens
+        })
+      });
+      
+      if (!response.ok) {
+        throw new Error(`Responses API error: ${response.status} ${response.statusText}`);
+      }
+      
+      if (this.config.backgroundMode) {
+        // Background mode: return job ID for polling
+        const data = await response.json();
+        log.info('Background reasoning job created', {
+          jobId: data.job_id,
+          estimatedTime: data.estimated_completion_time
+        });
+        
+        return {
+          jobId: data.job_id,
+          status: 'background',
+          response: ''
+        };
+      } else {
+        // Streaming mode: process SSE stream
+        return await this.processStream(response, callbacks, startTime);
+      }
+    } catch (error) {
+      log.error('Responses API request failed', {
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  /**
+   * Process SSE stream from Responses API
+   */
+  private async processStream(
+    response: Response,
+    callbacks: StreamingCallbacks | undefined,
+    startTime: number
+  ): Promise<ResponsesAPIResult> {
+    const reader = response.body?.getReader();
+    if (!reader) {
+      throw new Error('No response body');
+    }
+    
+    const decoder = new TextDecoder();
+    const reasoning: string[] = [];
+    let responseText = '';
+    let reasoningTokens = 0;
+    let isComplete = false;
+    
+    try {
+      while (!isComplete) {
+        const { done, value } = await reader.read();
+        if (done) break;
+        
+        const chunk = decoder.decode(value);
+        const lines = chunk.split('\n');
+        
+        for (const line of lines) {
+          if (line.startsWith('data: ')) {
+            const data = line.slice(6);
+            if (data === '[DONE]') {
+              isComplete = true;
+              break;
+            }
+            
+            try {
+              const event = JSON.parse(data);
+              
+              // Handle different event types
+              switch (event.type) {
+                case 'reasoning_step':
+                  reasoning.push(event.content);
+                  reasoningTokens += event.tokens || 0;
+                  
+                  if (callbacks?.onReasoning) {
+                    callbacks.onReasoning(event.content);
+                  }
+                  
+                  if (callbacks?.onProgress) {
+                    callbacks.onProgress({
+                      type: 'reasoning',
+                      content: event.content,
+                      timestamp: Date.now(),
+                      metadata: {
+                        step: event.step_number,
+                        tokens: event.tokens
+                      }
+                    });
+                  }
+                  break;
+                  
+                case 'response_delta':
+                  responseText += event.content;
+                  
+                  if (callbacks?.onProgress) {
+                    callbacks.onProgress({
+                      type: 'token',
+                      content: event.content,
+                      timestamp: Date.now()
+                    });
+                  }
+                  break;
+                  
+                case 'tool_call':
+                  log.debug('Tool call in reasoning', {
+                    tool: event.tool_name,
+                    arguments: event.arguments
+                  });
+                  
+                  if (callbacks?.onProgress) {
+                    callbacks.onProgress({
+                      type: 'tool_call',
+                      content: JSON.stringify(event),
+                      timestamp: Date.now(),
+                      metadata: event
+                    });
+                  }
+                  break;
+                  
+                case 'finish':
+                  // Calculate thinking time for onFinish callback
+                  
+                  if (callbacks?.onFinish) {
+                    callbacks.onFinish({
+                      text: responseText,
+                      usage: {
+                        promptTokens: event.usage?.prompt_tokens || 0,
+                        completionTokens: event.usage?.completion_tokens || 0,
+                        totalTokens: event.usage?.total_tokens || 0,
+                        reasoningTokens: reasoningTokens,
+                        totalCost: this.calculateCost(event.usage, reasoningTokens, Date.now() - startTime)
+                      },
+                      finishReason: event.finish_reason || 'stop'
+                    });
+                  }
+                  break;
+              }
+            } catch (e) {
+              log.warn('Failed to parse SSE event', { 
+                error: e instanceof Error ? e.message : String(e),
+                data 
+              });
+            }
+          }
+        }
+      }
+    } finally {
+      reader.releaseLock();
+    }
+    
+    const thinkingTime = Date.now() - startTime;
+    
+    log.info('Responses API stream completed', {
+      model: this.config.modelId,
+      reasoningSteps: reasoning.length,
+      reasoningTokens,
+      thinkingTimeMs: thinkingTime,
+      responseLength: responseText.length
+    });
+    
+    return {
+      status: 'completed',
+      reasoning,
+      thinkingTime,
+      reasoningTokens,
+      response: responseText
+    };
+  }
+  
+  /**
+   * Poll for background job status
+   */
+  async getJobStatus(jobId: string): Promise<ResponsesAPIResult> {
+    log.debug('Checking background job status', { jobId });
+    
+    const response = await fetch(`${this.baseURL}/jobs/${jobId}`, {
+      headers: {
+        'Authorization': `Bearer ${this.apiKey}`,
+        'OpenAI-Beta': 'responses-api-v1'
+      }
+    });
+    
+    if (!response.ok) {
+      throw new Error(`Failed to get job status: ${response.status}`);
+    }
+    
+    const data = await response.json();
+    
+    return {
+      jobId,
+      status: data.status,
+      reasoning: data.reasoning_steps,
+      thinkingTime: data.thinking_time_ms,
+      reasoningTokens: data.reasoning_tokens,
+      response: data.response || ''
+    };
+  }
+  
+  /**
+   * Calculate cost for reasoning models
+   */
+  private calculateCost(
+    usage: { prompt_tokens?: number; completion_tokens?: number } | undefined,
+    reasoningTokens: number,
+    thinkingTimeMs?: number
+  ): number {
+    if (!usage) return 0;
+    
+    // Pricing for reasoning models (estimated)
+    const prices = {
+      'o3': { input: 0.00015, output: 0.0006, reasoning: 0.0003 },
+      'o4': { input: 0.0002, output: 0.0008, reasoning: 0.0004 },
+      'gpt-5': { input: 0.00001, output: 0.00003, reasoning: 0.00002 }
+    };
+    
+    const modelPrefix = this.config.modelId.split('-')[0];
+    const pricing = prices[modelPrefix as keyof typeof prices] || prices['gpt-5'];
+    
+    const inputCost = (usage.prompt_tokens || 0) * pricing.input / 1000;
+    const outputCost = (usage.completion_tokens || 0) * pricing.output / 1000;
+    const reasoningCost = reasoningTokens * pricing.reasoning / 1000;
+    
+    // Add small cost for thinking time on reasoning models (optional)
+    const thinkingCost = thinkingTimeMs ? (thinkingTimeMs / 1000) * 0.0001 : 0;
+    
+    return inputCost + outputCost + reasoningCost + thinkingCost;
+  }
+}
+
+/**
+ * Integration helper for the unified streaming service
+ */
+export async function streamWithResponsesAPI(
+  request: StreamRequest,
+  callbacks?: StreamingCallbacks
+): Promise<StreamResponse> {
+  const client = createResponsesAPIClient({
+    apiKey: process.env.OPENAI_API_KEY || '',
+    modelId: request.modelId,
+    reasoningEffort: request.options?.reasoningEffort,
+    backgroundMode: request.options?.backgroundMode,
+    streamReasoningSummaries: true,
+    preserveReasoningItems: true,
+    maxReasoningTokens: 10000
+  });
+  
+  const messages = request.messages.map(msg => {
+    // Extract text content from parts if available
+    let content = '';
+    if ('parts' in msg && Array.isArray(msg.parts)) {
+      const textParts = (msg.parts as Array<{ type?: string; text?: string }>)
+        .filter(part => part.type === 'text')
+        .map(part => part.text || '');
+      content = textParts.join(' ');
+    } else if ('content' in msg) {
+      content = typeof msg.content === 'string' ? msg.content : JSON.stringify(msg.content);
+    }
+    
+    return {
+      role: msg.role,
+      content
+    };
+  });
+  
+  const result = await client.stream(messages, callbacks);
+  
+  // Convert to StreamResponse format
+  return {
+    result: {
+      toDataStreamResponse: () => new Response(result.response),
+      toUIMessageStreamResponse: () => new Response(result.response),
+      usage: Promise.resolve({
+        totalTokens: result.reasoningTokens || 0,
+        reasoningTokens: result.reasoningTokens
+      })
+    },
+    requestId: crypto.randomUUID(),
+    capabilities: {
+      supportsReasoning: true,
+      supportsThinking: false,
+      supportedResponseModes: ['standard', 'flex', 'priority'],
+      supportsBackgroundMode: true,
+      supportedTools: ['web_search', 'code_interpreter'],
+      typicalLatencyMs: 10000,
+      maxTimeoutMs: 600000
+    },
+    telemetryConfig: {
+      isEnabled: false,
+      functionId: 'responses-api',
+      metadata: {},
+      recordInputs: false,
+      recordOutputs: false
+    }
+  };
+}
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/azure-adapter.ts b/lib/streaming/provider-adapters/azure-adapter.ts
new file mode 100644
index 00000000..ef5cc64d
--- /dev/null
+++ b/lib/streaming/provider-adapters/azure-adapter.ts
@@ -0,0 +1,190 @@
+import { createAzure } from '@ai-sdk/azure';
+import { createLogger } from '@/lib/logger';
+import { Settings } from '@/lib/settings-manager';
+import { ErrorFactories } from '@/lib/error-utils';
+import { BaseProviderAdapter } from './base-adapter';
+import type { StreamingCallbacks } from '../types';
+import type { ProviderCapabilities, StreamRequest } from '../types';
+
+const log = createLogger({ module: 'azure-adapter' });
+
+/**
+ * Azure OpenAI provider adapter
+ * Supports Azure-hosted OpenAI models with enterprise features
+ */
+export class AzureAdapter extends BaseProviderAdapter {
+  protected providerName = 'azure';
+  
+  async createModel(modelId: string) {
+    try {
+      const config = await Settings.getAzureOpenAI();
+      if (!config.key || !config.resourceName) {
+        log.error('Azure OpenAI not configured');
+        throw ErrorFactories.sysConfigurationError('Azure OpenAI not configured');
+      }
+      
+      log.debug(`Creating Azure model: ${modelId}`, { 
+        modelId,
+        resourceName: config.resourceName
+      });
+      
+      const azure = createAzure({
+        apiKey: config.key,
+        resourceName: config.resourceName
+      });
+      
+      return azure(modelId);
+      
+    } catch (error) {
+      log.error('Failed to create Azure model', {
+        modelId,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  getCapabilities(modelId: string): ProviderCapabilities {
+    // Azure typically hosts OpenAI models, so capabilities are similar
+    // but may have enterprise-specific differences
+    
+    // GPT-4 models on Azure
+    if (this.matchesPattern(modelId, ['gpt-4*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: ['code_interpreter'], // If enabled in Azure deployment
+        typicalLatencyMs: 2500, // Slightly higher latency than OpenAI direct
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.00003, // Azure pricing
+        costPerOutputToken: 0.00006
+      };
+    }
+    
+    // GPT-3.5 models on Azure
+    if (this.matchesPattern(modelId, ['gpt-35*', 'gpt-3.5*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 1500,
+        maxTimeoutMs: 30000, // 30 seconds
+        costPerInputToken: 0.0000005,
+        costPerOutputToken: 0.0000015
+      };
+    }
+    
+    // Legacy text models on Azure
+    if (this.matchesPattern(modelId, ['text-davinci*', 'text-curie*', 'text-babbage*', 'text-ada*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 2000,
+        maxTimeoutMs: 30000, // 30 seconds
+        costPerInputToken: 0.000002,
+        costPerOutputToken: 0.000002
+      };
+    }
+    
+    // Default for unknown Azure models
+    return this.getDefaultCapabilities();
+  }
+  
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown> {
+    const baseOptions = super.getProviderOptions(modelId, options);
+    
+    // Add Azure-specific options
+    const azureOptions: Record<string, unknown> = {
+      ...baseOptions
+    };
+    
+    // Azure-specific configurations
+    azureOptions.azure = {
+      // Enterprise features
+      contentFiltering: true, // Azure has built-in content filtering
+      dataProcessingOptOut: true, // For enterprise privacy
+      
+      // Performance configurations
+      deploymentRegion: process.env.AZURE_OPENAI_REGION || 'eastus'
+    };
+    
+    return azureOptions;
+  }
+  
+  supportsModel(modelId: string): boolean {
+    const supportedPatterns = [
+      'gpt-3.5*',
+      'gpt-35*',
+      'gpt-4*',
+      'text-davinci*',
+      'text-curie*',
+      'text-babbage*',
+      'text-ada*',
+      'code-davinci*',
+      'code-cushman*'
+    ];
+    
+    return this.matchesPattern(modelId, supportedPatterns);
+  }
+  
+  protected async handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+      contentFilterResults?: unknown;
+    },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    await super.handleFinish(data, callbacks);
+    
+    // Handle Azure-specific content filtering results
+    if (data.contentFilterResults && callbacks.onProgress) {
+      callbacks.onProgress({
+        type: 'tool_result',
+        content: `Content filtering: ${JSON.stringify(data.contentFilterResults)}`,
+        timestamp: Date.now(),
+        metadata: { 
+          tool: 'content_filter',
+          azure: true
+        }
+      });
+    }
+  }
+  
+  protected handleError(error: Error, callbacks: StreamingCallbacks): void {
+    super.handleError(error, callbacks);
+    
+    // Handle Azure-specific errors
+    if (error.message.includes('content_filter')) {
+      log.warn('Azure content filter triggered', {
+        error: error.message
+      });
+    }
+    
+    if (error.message.includes('quota_exceeded')) {
+      log.warn('Azure quota exceeded', {
+        error: error.message
+      });
+    }
+    
+    if (error.message.includes('deployment_not_found')) {
+      log.error('Azure deployment not found', {
+        error: error.message
+      });
+    }
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/base-adapter.ts b/lib/streaming/provider-adapters/base-adapter.ts
new file mode 100644
index 00000000..889f5099
--- /dev/null
+++ b/lib/streaming/provider-adapters/base-adapter.ts
@@ -0,0 +1,267 @@
+import { streamText, consumeStream, type LanguageModel, type CoreMessage } from 'ai';
+import { createLogger } from '@/lib/logger';
+import type { 
+  ProviderAdapter, 
+  ProviderCapabilities, 
+  StreamConfig, 
+  StreamingCallbacks,
+  StreamRequest 
+} from '../types';
+
+const log = createLogger({ module: 'base-provider-adapter' });
+
+/**
+ * Base class for all provider adapters
+ * Provides common functionality and interface implementation
+ */
+export abstract class BaseProviderAdapter implements ProviderAdapter {
+  protected abstract providerName: string;
+  
+  /**
+   * Create a model instance for this provider
+   * Must be implemented by each provider
+   */
+  abstract createModel(modelId: string, options?: StreamRequest['options']): Promise<LanguageModel>;
+  
+  /**
+   * Get capabilities for a specific model
+   * Must be implemented by each provider
+   */
+  abstract getCapabilities(modelId: string): ProviderCapabilities;
+  
+  /**
+   * Get provider-specific options for streaming
+   * Can be overridden by specific providers
+   */
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown> {
+    const baseOptions: Record<string, unknown> = {};
+    
+    // Add common options
+    if (options?.reasoningEffort) {
+      baseOptions.reasoningEffort = options.reasoningEffort;
+    }
+    
+    if (options?.responseMode) {
+      baseOptions.responseMode = options.responseMode;
+    }
+    
+    if (options?.backgroundMode) {
+      baseOptions.backgroundMode = options.backgroundMode;
+    }
+    
+    return baseOptions;
+  }
+  
+  /**
+   * Stream with provider-specific enhancements
+   * Base implementation using AI SDK streamText
+   * Can be overridden for provider-specific features
+   */
+  async streamWithEnhancements(
+    config: StreamConfig,
+    callbacks: StreamingCallbacks
+  ): Promise<{
+    toDataStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    usage: Promise<{
+      totalTokens?: number;
+      promptTokens?: number;
+      completionTokens?: number;
+      reasoningTokens?: number;
+      totalCost?: number;
+    }>;
+  }> {
+    const logger = createLogger({ 
+      module: `${this.providerName}-adapter`,
+      requestId: config.experimental_telemetry?.metadata?.['request.id'] as string | undefined
+    });
+    
+    logger.debug('Starting stream with enhancements', {
+      provider: this.providerName,
+      hasModel: !!config.model,
+      messageCount: config.messages.length,
+      hasSystem: !!config.system,
+      hasTelemetry: !!config.experimental_telemetry?.isEnabled
+    });
+    
+    try {
+      // Create enhanced configuration
+      const enhancedConfig = this.enhanceStreamConfig(config);
+      
+      // Start streaming with AI SDK
+      const result = streamText({
+        model: enhancedConfig.model,
+        messages: enhancedConfig.messages as CoreMessage[],
+        system: enhancedConfig.system,
+        temperature: enhancedConfig.temperature,
+        // eslint-disable-next-line @typescript-eslint/no-explicit-any
+        experimental_telemetry: enhancedConfig.experimental_telemetry as any,
+        onFinish: async (event) => {
+          logger.info('streamText onFinish triggered', {
+            provider: this.providerName,
+            hasText: !!event.text,
+            hasUsage: !!event.usage,
+            finishReason: event.finishReason,
+            textLength: event.text?.length || 0
+          });
+          
+          // Transform to our expected format
+          const transformedData = {
+            text: event.text || '',
+            usage: event.usage ? {
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              promptTokens: (event.usage as any).promptTokens || 0,
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              completionTokens: (event.usage as any).completionTokens || 0,
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              totalTokens: (event.usage as any).totalTokens || 0
+            } : undefined,
+            finishReason: event.finishReason || 'stop'
+          };
+          
+          // Call provider-specific finish handler
+          await this.handleFinish(transformedData, callbacks);
+          
+          // Call user's finish callback
+          if (callbacks.onFinish) {
+            logger.info('Calling user onFinish callback from streamText', { 
+              hasCallback: true,
+              textLength: event.text?.length || 0 
+            });
+            await callbacks.onFinish(transformedData);
+          }
+        },
+        onError: (event) => {
+          const error = event.error instanceof Error ? event.error : new Error(String(event.error));
+          
+          logger.error('Stream error', {
+            provider: this.providerName,
+            error: error.message
+          });
+          
+          // Call provider-specific error handler
+          this.handleError(error, callbacks);
+          
+          // Call user's error callback
+          if (callbacks.onError) {
+            callbacks.onError(error);
+          }
+        }
+      });
+      
+      // Handle streaming chunks for progress tracking
+      this.handleStreamProgress(result, callbacks);
+      
+      return {
+        toDataStreamResponse: (options?: { headers?: Record<string, string> }) => 
+          result.toUIMessageStreamResponse ? result.toUIMessageStreamResponse(options) : result.toTextStreamResponse(options),
+        toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => 
+          result.toUIMessageStreamResponse ? result.toUIMessageStreamResponse(options) : result.toTextStreamResponse(options),
+        usage: result.usage
+      };
+      
+    } catch (error) {
+      logger.error('Failed to start stream', {
+        provider: this.providerName,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  /**
+   * Validate if this adapter supports the given model
+   * Must be implemented by each provider
+   */
+  abstract supportsModel(modelId: string): boolean;
+  
+  /**
+   * Enhance the stream configuration with provider-specific options
+   * Can be overridden by specific providers
+   */
+  protected enhanceStreamConfig(config: StreamConfig): StreamConfig {
+    return {
+      model: config.model,
+      messages: config.messages,
+      system: config.system,
+      maxTokens: config.maxTokens,
+      temperature: config.temperature,
+      experimental_telemetry: config.experimental_telemetry
+    };
+  }
+  
+  /**
+   * Handle streaming progress for callbacks
+   * Can be overridden by specific providers for custom progress handling
+   */
+  protected handleStreamProgress(result: unknown, callbacks: StreamingCallbacks): void {
+    // Base implementation - providers can override for custom progress tracking
+    if (callbacks.onProgress) {
+      // This would need to be implemented based on AI SDK streaming capabilities
+      // For now, this is a placeholder for the interface
+    }
+  }
+  
+  /**
+   * Handle stream finish event
+   * Can be overridden by specific providers
+   */
+  protected async handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+    },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    // Provider-specific handlers can override this to extract special content
+    // For example, Claude might extract thinking content
+  }
+  
+  /**
+   * Handle stream error event
+   * Can be overridden by specific providers
+   */
+  protected handleError(error: Error, callbacks: StreamingCallbacks): void {
+    // Base error handling - log the error
+    log.error(`${this.providerName} adapter error`, {
+      error: error.message,
+      provider: this.providerName
+    });
+  }
+  
+  /**
+   * Get default capabilities for unknown models
+   * Used as fallback when specific model capabilities are unknown
+   */
+  protected getDefaultCapabilities(): ProviderCapabilities {
+    return {
+      supportsReasoning: false,
+      supportsThinking: false,
+      supportedResponseModes: ['standard'],
+      supportsBackgroundMode: false,
+      supportedTools: [],
+      typicalLatencyMs: 2000,
+      maxTimeoutMs: 30000
+    };
+  }
+  
+  /**
+   * Check if a model ID matches a pattern
+   */
+  protected matchesPattern(modelId: string, patterns: string[]): boolean {
+    return patterns.some(pattern => {
+      if (pattern.includes('*')) {
+        const regex = new RegExp(pattern.replace(/\*/g, '.*'), 'i');
+        return regex.test(modelId);
+      }
+      return modelId.toLowerCase().includes(pattern.toLowerCase());
+    });
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/claude-adapter.ts b/lib/streaming/provider-adapters/claude-adapter.ts
new file mode 100644
index 00000000..08fe68f7
--- /dev/null
+++ b/lib/streaming/provider-adapters/claude-adapter.ts
@@ -0,0 +1,238 @@
+import { createAmazonBedrock } from '@ai-sdk/amazon-bedrock';
+import { createLogger } from '@/lib/logger';
+import { Settings } from '@/lib/settings-manager';
+import { BaseProviderAdapter } from './base-adapter';
+import type { StreamingCallbacks } from '../types';
+import type { ProviderCapabilities, StreamRequest, StreamConfig } from '../types';
+
+const log = createLogger({ module: 'claude-adapter' });
+
+/**
+ * Claude provider adapter (via Amazon Bedrock) with support for:
+ * - Claude 4 Opus/Sonnet with thinking capabilities
+ * - Extended thinking budgets (1024-6553 tokens)
+ * - Enhanced reasoning and chain-of-thought
+ */
+export class ClaudeAdapter extends BaseProviderAdapter {
+  protected providerName = 'amazon-bedrock';
+  
+  async createModel(modelId: string, options?: StreamRequest['options']) {
+    try {
+      const config = await Settings.getBedrock();
+      const isLambda = !!process.env.AWS_LAMBDA_FUNCTION_NAME;
+      
+      log.debug(`Creating Claude model: ${modelId}`, {
+        modelId,
+        hasAccessKey: !!config.accessKeyId,
+        hasSecretKey: !!config.secretAccessKey,
+        region: config.region || 'us-east-1',
+        isLambda,
+        thinkingBudget: options?.thinkingBudget
+      });
+      
+      const bedrockOptions: Parameters<typeof createAmazonBedrock>[0] = {
+        region: config.region || 'us-east-1'
+      };
+      
+      // Use explicit credentials for local development only
+      if (!isLambda && config.accessKeyId && config.secretAccessKey) {
+        log.debug('Using explicit credentials for local development');
+        bedrockOptions.accessKeyId = config.accessKeyId;
+        bedrockOptions.secretAccessKey = config.secretAccessKey;
+      }
+      
+      const bedrock = createAmazonBedrock(bedrockOptions);
+      return bedrock(modelId);
+      
+    } catch (error) {
+      log.error('Failed to create Claude model', {
+        modelId,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  getCapabilities(modelId: string): ProviderCapabilities {
+    // Claude 4 models with thinking capabilities
+    if (this.matchesPattern(modelId, ['claude-4*', 'anthropic.claude-4*'])) {
+      return {
+        supportsReasoning: true,
+        supportsThinking: true,
+        maxThinkingTokens: 6553, // Maximum thinking budget for Claude 4
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false, // Claude doesn't support background mode yet
+        supportedTools: [],
+        typicalLatencyMs: 3000,
+        maxTimeoutMs: 120000, // 2 minutes for thinking models
+        costPerInputToken: 0.000015,
+        costPerOutputToken: 0.000075
+      };
+    }
+    
+    // Claude 3.5 Sonnet
+    if (this.matchesPattern(modelId, ['claude-3-5*', 'anthropic.claude-3-5*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 2000,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.000003,
+        costPerOutputToken: 0.000015
+      };
+    }
+    
+    // Claude 3 models (Opus, Sonnet, Haiku)
+    if (this.matchesPattern(modelId, ['claude-3*', 'anthropic.claude-3*'])) {
+      const isOpus = this.matchesPattern(modelId, ['*opus*']);
+      const isHaiku = this.matchesPattern(modelId, ['*haiku*']);
+      
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: isHaiku ? 1000 : isOpus ? 3000 : 2000,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: isOpus ? 0.000015 : isHaiku ? 0.00000025 : 0.000003,
+        costPerOutputToken: isOpus ? 0.000075 : isHaiku ? 0.00000125 : 0.000015
+      };
+    }
+    
+    // Claude 2 models
+    if (this.matchesPattern(modelId, ['claude-2*', 'anthropic.claude-2*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 2500,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.000008,
+        costPerOutputToken: 0.000024
+      };
+    }
+    
+    // Default for unknown Claude models
+    return this.getDefaultCapabilities();
+  }
+  
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown> {
+    const baseOptions = super.getProviderOptions(modelId, options);
+    
+    // Add Claude-specific options
+    const claudeOptions: Record<string, unknown> = {
+      ...baseOptions
+    };
+    
+    // Configure thinking budget for Claude 4 models
+    if (this.supportsThinking(modelId)) {
+      claudeOptions.anthropic = {
+        // Thinking configuration
+        thinkingBudget: this.getThinkingBudget(options?.thinkingBudget),
+        enableThinking: true,
+        streamThinking: true // Stream thinking content for transparency
+      };
+    }
+    
+    return claudeOptions;
+  }
+  
+  protected enhanceStreamConfig(config: StreamConfig): StreamConfig {
+    const enhanced = super.enhanceStreamConfig(config);
+    
+    // Add Claude-specific enhancements
+    if (config.providerOptions?.anthropic) {
+      enhanced.providerOptions = config.providerOptions;
+    }
+    
+    return enhanced;
+  }
+  
+  supportsModel(modelId: string): boolean {
+    const supportedPatterns = [
+      'claude-*',
+      'anthropic.claude-*'
+    ];
+    
+    return this.matchesPattern(modelId, supportedPatterns);
+  }
+  
+  /**
+   * Check if model supports thinking capabilities
+   */
+  private supportsThinking(modelId: string): boolean {
+    return this.matchesPattern(modelId, ['claude-4*', 'anthropic.claude-4*']);
+  }
+  
+  /**
+   * Get appropriate thinking budget based on user preference and model limits
+   */
+  private getThinkingBudget(requestedBudget?: number): number {
+    // Default to medium thinking budget
+    const defaultBudget = 3000;
+    
+    if (!requestedBudget) {
+      return defaultBudget;
+    }
+    
+    // Clamp to Claude 4 limits (1024-6553 tokens)
+    return Math.max(1024, Math.min(6553, requestedBudget));
+  }
+  
+  protected async handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        thinkingTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+      thinking?: string;
+      model?: string;
+    },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    await super.handleFinish(data, callbacks);
+    
+    // Handle Claude-specific thinking content
+    if (data.thinking && callbacks.onThinking) {
+      callbacks.onThinking(data.thinking);
+    }
+    
+    // Log thinking token usage for cost tracking
+    if (data.usage?.thinkingTokens) {
+      log.debug('Claude thinking tokens used', {
+        thinkingTokens: data.usage.thinkingTokens,
+        totalTokens: data.usage.totalTokens,
+        model: data.model
+      });
+    }
+  }
+  
+  protected handleError(error: Error, callbacks: StreamingCallbacks): void {
+    super.handleError(error, callbacks);
+    
+    // Handle Claude-specific errors
+    if (error.message.includes('thinking_budget_exceeded')) {
+      log.warn('Claude thinking budget exceeded', {
+        error: error.message
+      });
+    }
+    
+    if (error.message.includes('content_policy_violation')) {
+      log.warn('Claude content policy violation', {
+        error: error.message
+      });
+    }
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/gemini-adapter.ts b/lib/streaming/provider-adapters/gemini-adapter.ts
new file mode 100644
index 00000000..d57d8ff3
--- /dev/null
+++ b/lib/streaming/provider-adapters/gemini-adapter.ts
@@ -0,0 +1,210 @@
+import { google } from '@ai-sdk/google';
+import { createLogger } from '@/lib/logger';
+import { Settings } from '@/lib/settings-manager';
+import { ErrorFactories } from '@/lib/error-utils';
+import { BaseProviderAdapter } from './base-adapter';
+import type { StreamingCallbacks } from '../types';
+import type { ProviderCapabilities, StreamRequest } from '../types';
+
+const log = createLogger({ module: 'gemini-adapter' });
+
+/**
+ * Google Gemini provider adapter with support for:
+ * - Gemini 2.5 with extended reasoning capabilities
+ * - Gemini 1.5 Pro/Flash models
+ * - Extended context windows and multimodal capabilities
+ */
+export class GeminiAdapter extends BaseProviderAdapter {
+  protected providerName = 'google';
+  
+  async createModel(modelId: string) {
+    try {
+      const apiKey = await Settings.getGoogleAI();
+      if (!apiKey) {
+        log.error('Google API key not configured');
+        throw ErrorFactories.sysConfigurationError('Google API key not configured');
+      }
+      
+      log.debug(`Creating Gemini model: ${modelId}`, { modelId });
+      
+      // Set environment variable for Google SDK
+      process.env.GOOGLE_GENERATIVE_AI_API_KEY = apiKey;
+      
+      return google(modelId);
+      
+    } catch (error) {
+      log.error('Failed to create Gemini model', {
+        modelId,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  getCapabilities(modelId: string): ProviderCapabilities {
+    // Gemini 2.5 models with enhanced reasoning
+    if (this.matchesPattern(modelId, ['gemini-2.5*', 'models/gemini-2.5*'])) {
+      return {
+        supportsReasoning: true,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: ['code_execution'],
+        typicalLatencyMs: 2500,
+        maxTimeoutMs: 90000, // 1.5 minutes for reasoning models
+        costPerInputToken: 0.000002, // Estimated
+        costPerOutputToken: 0.000008
+      };
+    }
+    
+    // Gemini 1.5 Pro
+    if (this.matchesPattern(modelId, ['gemini-1.5-pro*', 'models/gemini-1.5-pro*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: ['code_execution'],
+        typicalLatencyMs: 2000,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.00000125,
+        costPerOutputToken: 0.000005
+      };
+    }
+    
+    // Gemini 1.5 Flash
+    if (this.matchesPattern(modelId, ['gemini-1.5-flash*', 'models/gemini-1.5-flash*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: ['code_execution'],
+        typicalLatencyMs: 1000, // Faster than Pro
+        maxTimeoutMs: 30000, // 30 seconds
+        costPerInputToken: 0.000000075,
+        costPerOutputToken: 0.0000003
+      };
+    }
+    
+    // Gemini 1.0 Pro
+    if (this.matchesPattern(modelId, ['gemini-pro*', 'gemini-1.0-pro*', 'models/gemini-pro*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 2500,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.0000005,
+        costPerOutputToken: 0.0000015
+      };
+    }
+    
+    // Default for unknown Gemini models
+    return this.getDefaultCapabilities();
+  }
+  
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown> {
+    const baseOptions = super.getProviderOptions(modelId, options);
+    
+    // Add Gemini-specific options
+    const geminiOptions: Record<string, unknown> = {
+      ...baseOptions
+    };
+    
+    // Configure for enhanced reasoning models
+    if (this.supportsEnhancedReasoning(modelId)) {
+      geminiOptions.google = {
+        // Reasoning configuration for Gemini 2.5
+        enhancedReasoning: true,
+        reasoningDepth: options?.reasoningEffort === 'high' ? 'deep' : 
+                        options?.reasoningEffort === 'low' ? 'shallow' : 'medium',
+        
+        // Code execution capabilities
+        enableCodeExecution: options?.enableCodeInterpreter || false
+      };
+    }
+    
+    return geminiOptions;
+  }
+  
+  supportsModel(modelId: string): boolean {
+    const supportedPatterns = [
+      'gemini-*',
+      'models/gemini-*',
+      'gemini-pro*',
+      'gemini-1.0*',
+      'gemini-1.5*',
+      'gemini-2.0*',
+      'gemini-2.5*'
+    ];
+    
+    return this.matchesPattern(modelId, supportedPatterns);
+  }
+  
+  /**
+   * Check if model supports enhanced reasoning
+   */
+  private supportsEnhancedReasoning(modelId: string): boolean {
+    return this.matchesPattern(modelId, ['gemini-2.5*', 'models/gemini-2.5*']);
+  }
+  
+  protected async handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+      reasoning?: string;
+      codeExecutionResults?: unknown;
+    },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    await super.handleFinish(data, callbacks);
+    
+    // Handle Gemini-specific reasoning content
+    if (data.reasoning && callbacks.onReasoning) {
+      callbacks.onReasoning(data.reasoning);
+    }
+    
+    // Handle code execution results
+    if (data.codeExecutionResults && callbacks.onProgress) {
+      callbacks.onProgress({
+        type: 'tool_result',
+        content: JSON.stringify(data.codeExecutionResults),
+        timestamp: Date.now(),
+        metadata: { tool: 'code_execution' }
+      });
+    }
+  }
+  
+  protected handleError(error: Error, callbacks: StreamingCallbacks): void {
+    super.handleError(error, callbacks);
+    
+    // Handle Gemini-specific errors
+    if (error.message.includes('SAFETY')) {
+      log.warn('Gemini safety filter triggered', {
+        error: error.message
+      });
+    }
+    
+    if (error.message.includes('QUOTA_EXCEEDED')) {
+      log.warn('Gemini quota exceeded', {
+        error: error.message
+      });
+    }
+    
+    if (error.message.includes('RECITATION')) {
+      log.warn('Gemini recitation filter triggered', {
+        error: error.message
+      });
+    }
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/index.ts b/lib/streaming/provider-adapters/index.ts
new file mode 100644
index 00000000..fc5f2c06
--- /dev/null
+++ b/lib/streaming/provider-adapters/index.ts
@@ -0,0 +1,62 @@
+import { createLogger } from '@/lib/logger';
+import type { ProviderAdapter, ProviderCapabilities } from '../types';
+import { OpenAIAdapter } from './openai-adapter';
+import { ClaudeAdapter } from './claude-adapter';
+import { GeminiAdapter } from './gemini-adapter';
+import { AzureAdapter } from './azure-adapter';
+
+const log = createLogger({ module: 'provider-adapters' });
+
+// Registry of all available provider adapters
+const adapters = new Map<string, ProviderAdapter>([
+  ['openai', new OpenAIAdapter()],
+  ['amazon-bedrock', new ClaudeAdapter()],
+  ['google', new GeminiAdapter()],
+  ['azure', new AzureAdapter()]
+]);
+
+/**
+ * Get the appropriate provider adapter for the given provider
+ */
+export async function getProviderAdapter(provider: string): Promise<ProviderAdapter> {
+  const normalizedProvider = provider.toLowerCase();
+  const adapter = adapters.get(normalizedProvider);
+  
+  if (!adapter) {
+    log.error('Unknown provider', { provider });
+    throw new Error(`Unknown provider: ${provider}`);
+  }
+  
+  return adapter;
+}
+
+/**
+ * Get capabilities for a specific model across all providers
+ */
+export async function getModelCapabilities(provider: string, modelId: string): Promise<ProviderCapabilities> {
+  const adapter = await getProviderAdapter(provider);
+  return adapter.getCapabilities(modelId);
+}
+
+/**
+ * Check if a model is supported by any provider
+ */
+export async function isModelSupported(provider: string, modelId: string): Promise<boolean> {
+  try {
+    const adapter = await getProviderAdapter(provider);
+    return adapter.supportsModel(modelId);
+  } catch {
+    return false;
+  }
+}
+
+/**
+ * Get list of all supported providers
+ */
+export function getSupportedProviders(): string[] {
+  return Array.from(adapters.keys());
+}
+
+// Re-export types and base classes
+export type { ProviderAdapter, ProviderCapabilities } from '../types';
+export { BaseProviderAdapter } from './base-adapter';
\ No newline at end of file
diff --git a/lib/streaming/provider-adapters/openai-adapter.ts b/lib/streaming/provider-adapters/openai-adapter.ts
new file mode 100644
index 00000000..aabdc3e9
--- /dev/null
+++ b/lib/streaming/provider-adapters/openai-adapter.ts
@@ -0,0 +1,404 @@
+import { createOpenAI } from '@ai-sdk/openai';
+import { streamText, consumeStream, type CoreMessage } from 'ai';
+import { createLogger } from '@/lib/logger';
+import { Settings } from '@/lib/settings-manager';
+import { ErrorFactories } from '@/lib/error-utils';
+import { BaseProviderAdapter } from './base-adapter';
+import type { StreamingCallbacks, StreamConfig, ProviderCapabilities, StreamRequest } from '../types';
+
+const log = createLogger({ module: 'openai-adapter' });
+
+/**
+ * OpenAI provider adapter with support for:
+ * - GPT-5, GPT-4, GPT-3.5, o3, o4 models
+ * - OpenAI Responses API for reasoning models
+ * - Background mode for long-running reasoning
+ * - Enhanced reasoning support and token persistence
+ */
+export class OpenAIAdapter extends BaseProviderAdapter {
+  protected providerName = 'openai';
+  
+  async createModel(modelId: string, options?: StreamRequest['options']) {
+    try {
+      const apiKey = await Settings.getOpenAI();
+      if (!apiKey || typeof apiKey !== 'string' || apiKey.trim() === '') {
+        log.error('OpenAI API key not configured or invalid');
+        throw ErrorFactories.sysConfigurationError('OpenAI API key not configured');
+      }
+      
+      log.debug(`Creating OpenAI model: ${modelId}`, {
+        modelId,
+        useResponsesAPI: true, // Always use Responses API
+        backgroundMode: options?.backgroundMode
+      });
+      
+      const openai = createOpenAI({ apiKey });
+      
+      // Always use Responses API for all OpenAI models
+      log.info('Using OpenAI Responses API', {
+        modelId,
+        reasoningEffort: options?.reasoningEffort || 'medium',
+        backgroundMode: options?.backgroundMode || false
+      });
+      
+      // Return a model configured for Responses API
+      // The Responses API supports advanced reasoning with structured outputs
+      const model = openai(modelId);
+      
+      // Store metadata for later use in getProviderOptions
+      // Type assertion needed for metadata storage
+      const modelWithMetadata = model as typeof model & {
+        __responsesAPI: boolean;
+        __reasoningEffort: 'minimal' | 'low' | 'medium' | 'high';
+        __backgroundMode: boolean;
+      };
+      modelWithMetadata.__responsesAPI = true;
+      modelWithMetadata.__reasoningEffort = options?.reasoningEffort || 'medium';
+      modelWithMetadata.__backgroundMode = options?.backgroundMode || false;
+      
+      return model;
+      
+    } catch (error) {
+      log.error('Failed to create OpenAI model', {
+        modelId,
+        error: error instanceof Error ? error.message : String(error)
+      });
+      throw error;
+    }
+  }
+  
+  getCapabilities(modelId: string): ProviderCapabilities {
+    // GPT-5 models
+    if (this.matchesPattern(modelId, ['gpt-5*', 'gpt-5-*'])) {
+      return {
+        supportsReasoning: true,
+        supportsThinking: false,
+        supportedResponseModes: ['standard', 'flex', 'priority'],
+        supportsBackgroundMode: true,
+        supportedTools: ['web_search', 'code_interpreter', 'image_generation'],
+        typicalLatencyMs: 3000,
+        maxTimeoutMs: 300000, // 5 minutes
+        costPerInputToken: 0.00001, // Estimated
+        costPerOutputToken: 0.00003,
+        costPerReasoningToken: 0.00002
+      };
+    }
+    
+    // o3/o4 reasoning models
+    if (this.matchesPattern(modelId, ['o3*', 'o4*'])) {
+      return {
+        supportsReasoning: true,
+        supportsThinking: false,
+        supportedResponseModes: ['standard', 'flex', 'priority'],
+        supportsBackgroundMode: true,
+        supportedTools: ['web_search', 'code_interpreter'],
+        typicalLatencyMs: 10000, // Much slower for reasoning
+        maxTimeoutMs: 600000, // 10 minutes for complex reasoning
+        costPerInputToken: 0.00015, // Higher cost for reasoning models
+        costPerOutputToken: 0.0006,
+        costPerReasoningToken: 0.0003
+      };
+    }
+    
+    // GPT-4 models
+    if (this.matchesPattern(modelId, ['gpt-4*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: ['code_interpreter'],
+        typicalLatencyMs: 2000,
+        maxTimeoutMs: 60000, // 1 minute
+        costPerInputToken: 0.00003,
+        costPerOutputToken: 0.00006
+      };
+    }
+    
+    // GPT-3.5 models
+    if (this.matchesPattern(modelId, ['gpt-3.5*', 'gpt-35*'])) {
+      return {
+        supportsReasoning: false,
+        supportsThinking: false,
+        supportedResponseModes: ['standard'],
+        supportsBackgroundMode: false,
+        supportedTools: [],
+        typicalLatencyMs: 1000,
+        maxTimeoutMs: 30000, // 30 seconds
+        costPerInputToken: 0.0000005,
+        costPerOutputToken: 0.0000015
+      };
+    }
+    
+    // Default for unknown OpenAI models
+    return {
+      ...this.getDefaultCapabilities(),
+      supportedTools: ['code_interpreter']
+    };
+  }
+  
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown> {
+    const baseOptions = super.getProviderOptions(modelId, options);
+    
+    // Always configure for Responses API
+    const openaiOptions: Record<string, unknown> = {
+      ...baseOptions,
+      openai: {
+        // Reasoning configuration
+        reasoningEffort: options?.reasoningEffort || 'medium',
+        backgroundMode: options?.backgroundMode || false,
+        includeReasoningSummaries: true,
+        preserveReasoningItems: true, // For multi-turn conversations
+        
+        // Tool configuration
+        enableWebSearch: options?.enableWebSearch || false,
+        enableCodeInterpreter: options?.enableCodeInterpreter || false,
+        enableImageGeneration: options?.enableImageGeneration || false
+      }
+    };
+    
+    return openaiOptions;
+  }
+  
+  protected enhanceStreamConfig(config: StreamConfig): StreamConfig {
+    const enhanced = super.enhanceStreamConfig(config);
+    
+    // Add provider-specific options to providerOptions
+    if (config.providerOptions) {
+      enhanced.providerOptions = config.providerOptions;
+    }
+    
+    return enhanced;
+  }
+  
+  supportsModel(modelId: string): boolean {
+    const supportedPatterns = [
+      'gpt-3.5*',
+      'gpt-35*', 
+      'gpt-4*',
+      'gpt-5*',
+      'o3*',
+      'o4*',
+      'text-davinci*',
+      'text-curie*',
+      'text-babbage*',
+      'text-ada*'
+    ];
+    
+    return this.matchesPattern(modelId, supportedPatterns);
+  }
+  
+  /**
+   * Override streamWithEnhancements to implement OpenAI Responses API
+   */
+  async streamWithEnhancements(
+    config: StreamConfig,
+    callbacks: StreamingCallbacks
+  ): Promise<{
+    toDataStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    usage: Promise<{
+      totalTokens?: number;
+      promptTokens?: number;
+      completionTokens?: number;
+      reasoningTokens?: number;
+      totalCost?: number;
+    }>;
+  }> {
+    const logger = createLogger({ 
+      module: 'openai-adapter.streamWithEnhancements'
+    });
+    
+    // All OpenAI models use Responses API
+    const modelWithMetadata = config.model as typeof config.model & {
+      __responsesAPI?: boolean;
+      __reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high';
+      __backgroundMode?: boolean;
+    };
+    const isResponsesAPI = modelWithMetadata.__responsesAPI === true;
+    
+    if (isResponsesAPI) {
+      logger.info('Using OpenAI Responses API streaming', {
+        reasoningEffort: modelWithMetadata.__reasoningEffort,
+        backgroundMode: modelWithMetadata.__backgroundMode
+      });
+      
+      // Configure for Responses API
+      const enhancedConfig = {
+        ...config,
+        // Add Responses API specific parameters
+        providerOptions: {
+          ...config.providerOptions,
+          openai: {
+            // Reasoning configuration
+            reasoning_effort: modelWithMetadata.__reasoningEffort,
+            background_mode: modelWithMetadata.__backgroundMode,
+            stream_reasoning_summaries: true,
+            preserve_reasoning_items: true,
+            
+            // Response configuration
+            response_format: {
+              type: 'json_object' as const,
+              schema: {
+                reasoning_steps: 'array',
+                thinking_content: 'string',
+                final_answer: 'string'
+              }
+            },
+            
+            // Tool configuration for reasoning models
+            parallel_tool_calls: true,
+            tool_choice: 'auto' as const
+          }
+        }
+      };
+      
+      // Stream with Responses API enhancements
+      const result = streamText({
+        model: enhancedConfig.model,
+        messages: enhancedConfig.messages as CoreMessage[],
+        system: enhancedConfig.system,
+        temperature: enhancedConfig.temperature,
+        onFinish: async (event) => {
+          logger.info('OpenAI streamText onFinish triggered', {
+            hasText: !!event.text,
+            hasUsage: !!event.usage,
+            finishReason: event.finishReason,
+            textLength: event.text?.length || 0
+          });
+          
+          // Transform to our expected format
+          const transformedData = {
+            text: event.text || '',
+            usage: event.usage ? {
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              promptTokens: (event.usage as any).promptTokens || 0,
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              completionTokens: (event.usage as any).completionTokens || 0,
+              // eslint-disable-next-line @typescript-eslint/no-explicit-any
+              totalTokens: (event.usage as any).totalTokens || 0
+            } : undefined,
+            finishReason: event.finishReason || 'stop'
+          };
+          
+          // Call finish callbacks
+          if (callbacks.onFinish) {
+            logger.info('Calling onFinish callback from OpenAI adapter', {
+              hasCallback: true,
+              textLength: event.text?.length || 0
+            });
+            await callbacks.onFinish(transformedData);
+          }
+        }
+      });
+      
+      // Process reasoning content from the stream
+      this.processResponsesAPIStream(result, callbacks);
+      
+      return {
+        toDataStreamResponse: (options?: { headers?: Record<string, string> }) => 
+          result.toUIMessageStreamResponse ? result.toUIMessageStreamResponse(options) : result.toTextStreamResponse(options),
+        toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => 
+          result.toUIMessageStreamResponse ? result.toUIMessageStreamResponse(options) : result.toTextStreamResponse(options),
+        usage: result.usage
+      };
+    }
+    
+    // Fall back to base implementation for non-Responses API models
+    return super.streamWithEnhancements(config, callbacks);
+  }
+  
+  /**
+   * Process Responses API stream to extract reasoning content
+   */
+  private async processResponsesAPIStream(
+    result: { fullStream?: AsyncIterable<unknown> },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    const logger = createLogger({ module: 'openai-adapter.processResponsesAPIStream' });
+    
+    try {
+      // Process the full stream to extract reasoning
+      if (result.fullStream) {
+        for await (const part of result.fullStream) {
+        const typedPart = part as { type?: string; text?: string; toolName?: string; toolCallId?: string; totalUsage?: { reasoningTokens?: number; totalTokens?: number } };
+        switch (typedPart.type) {
+          case 'text-delta':
+            // Check if this is reasoning content
+            if (typedPart.text && typedPart.text.includes('[REASONING]')) {
+              const reasoning = typedPart.text.replace('[REASONING]', '').trim();
+              if (reasoning && callbacks.onReasoning) {
+                callbacks.onReasoning(reasoning);
+              }
+            }
+            break;
+            
+          case 'reasoning-delta':
+            // Native reasoning support (when AI SDK adds it)
+            if (typedPart.text && callbacks.onReasoning) {
+              callbacks.onReasoning(typedPart.text);
+            }
+            break;
+            
+          case 'tool-call':
+            // Handle tool calls in reasoning models
+            logger.debug('Tool call in reasoning model', {
+              toolName: typedPart.toolName,
+              toolCallId: typedPart.toolCallId
+            });
+            break;
+            
+          case 'finish':
+            // Extract final reasoning metrics
+            if (typedPart.totalUsage?.reasoningTokens) {
+              logger.info('Reasoning tokens used', {
+                reasoningTokens: typedPart.totalUsage.reasoningTokens,
+                totalTokens: typedPart.totalUsage.totalTokens
+              });
+            }
+            break;
+        }
+      }
+      }
+    } catch (error) {
+      logger.error('Error processing Responses API stream', {
+        error: error instanceof Error ? error.message : String(error)
+      });
+    }
+  }
+  
+  
+  protected async handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+      reasoning?: string;
+      backgroundJobId?: string;
+      backgroundJobStatus?: string;
+      model?: string;
+    },
+    callbacks: StreamingCallbacks
+  ): Promise<void> {
+    await super.handleFinish(data, callbacks);
+    
+    // Handle OpenAI-specific reasoning content
+    if (data.reasoning && callbacks.onReasoning) {
+      callbacks.onReasoning(data.reasoning);
+    }
+    
+    // Handle background job completion
+    if (data.backgroundJobId && data.backgroundJobStatus === 'completed') {
+      log.info('Background reasoning job completed', {
+        jobId: data.backgroundJobId,
+        model: data.model
+      });
+    }
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/telemetry-service.ts b/lib/streaming/telemetry-service.ts
new file mode 100644
index 00000000..a92bb01f
--- /dev/null
+++ b/lib/streaming/telemetry-service.ts
@@ -0,0 +1,341 @@
+import { createLogger } from '@/lib/logger';
+import { Settings } from '@/lib/settings-manager';
+import type { TelemetrySpan, TelemetryConfig } from './types';
+
+// OpenTelemetry types
+interface OTelTracer {
+  startSpan: (name: string, options?: Record<string, unknown>) => TelemetrySpan;
+}
+
+interface OTelMeter {
+  createCounter: (name: string, options?: { description?: string }) => {
+    add: (value: number, attributes?: Record<string, string | number>) => void;
+  };
+  createHistogram: (name: string, options?: { description?: string }) => {
+    record: (value: number, attributes?: Record<string, string | number>) => void;
+  };
+}
+
+// OpenTelemetry imports are optional - fail gracefully if not installed
+let trace: { getTracer: (name: string, version: string) => OTelTracer };
+let metrics: { getMeter: (name: string, version: string) => OTelMeter };
+
+// Initialize with no-op implementations
+trace = {
+  getTracer: () => ({
+    startSpan: () => ({
+      setAttributes: () => {},
+      addEvent: () => {},
+      recordException: () => {},
+      setStatus: () => {},
+      end: () => {}
+    })
+  })
+};
+metrics = {
+  getMeter: () => ({
+    createCounter: () => ({ add: () => {} }),
+    createHistogram: () => ({ record: () => {} })
+  })
+};
+
+// Try to load OpenTelemetry if available
+if (typeof window === 'undefined') { // Only on server side
+  try {
+    // Dynamic import for optional OpenTelemetry dependency
+    // eslint-disable-next-line @typescript-eslint/no-require-imports
+    const otelApi = require('@opentelemetry/api');
+    trace = otelApi.trace;
+    metrics = otelApi.metrics;
+  } catch {
+    // OpenTelemetry not installed - use no-op implementations
+  }
+}
+
+const log = createLogger({ module: 'telemetry-service' });
+
+export interface AITelemetryRequest {
+  functionId: string;
+  userId?: string;
+  sessionId?: string;
+  conversationId?: string | number;
+  modelId: string;
+  provider: string;
+  source: 'chat' | 'compare' | 'assistant_execution' | 'ai-helpers';
+  recordInputs?: boolean;
+  recordOutputs?: boolean;
+  customAttributes?: Record<string, string | number | boolean>;
+}
+
+/**
+ * Get telemetry configuration for AI operations
+ * Follows OpenTelemetry semantic conventions for AI/ML workloads
+ */
+export async function getTelemetryConfig(request: AITelemetryRequest): Promise<TelemetryConfig> {
+  try {
+    // Check if telemetry is enabled
+    const telemetryEnabled = process.env.TELEMETRY_ENABLED === 'true';
+    
+    if (!telemetryEnabled) {
+      return {
+        isEnabled: false,
+        functionId: request.functionId,
+        metadata: {},
+        recordInputs: false,
+        recordOutputs: false
+      };
+    }
+    
+    // Get telemetry settings from database/env
+    const settings = await getTelemetrySettings();
+    
+    if (!settings.enabled) {
+      return {
+        isEnabled: false,
+        functionId: request.functionId,
+        metadata: {},
+        recordInputs: false,
+        recordOutputs: false
+      };
+    }
+    
+    // Create tracer if available
+    const tracer = trace.getTracer('aistudio.ai', '1.0.0');
+    
+    // Build metadata following OpenTelemetry semantic conventions
+    const metadata = {
+      // Service attributes
+      'service.name': 'aistudio',
+      'service.version': process.env.APP_VERSION || '1.0.0',
+      'service.environment': process.env.NODE_ENV || 'development',
+      
+      // AI/ML specific attributes
+      'ai.model.id': request.modelId,
+      'ai.model.provider': request.provider,
+      'ai.request.source': request.source,
+      'ai.operation.name': request.functionId,
+      
+      // User context (if available and privacy allows)
+      ...(settings.recordUserContext && request.userId && {
+        'user.id': request.userId
+      }),
+      ...(request.sessionId && {
+        'ai.session.id': request.sessionId
+      }),
+      ...(request.conversationId && {
+        'ai.conversation.id': String(request.conversationId)
+      }),
+      
+      // Custom attributes
+      ...request.customAttributes,
+      
+      // Timestamps
+      'ai.request.timestamp': Date.now(),
+      'ai.request.timezone': Intl.DateTimeFormat().resolvedOptions().timeZone
+    };
+    
+    return {
+      isEnabled: true,
+      functionId: request.functionId,
+      metadata,
+      recordInputs: settings.recordInputs ?? request.recordInputs ?? true,
+      recordOutputs: settings.recordOutputs ?? request.recordOutputs ?? true,
+      tracer
+    };
+    
+  } catch (error) {
+    log.error('Failed to get telemetry config', { 
+      error: error instanceof Error ? error.message : String(error),
+      functionId: request.functionId
+    });
+    
+    // Return disabled config on error to avoid breaking AI operations
+    return {
+      isEnabled: false,
+      functionId: request.functionId,
+      metadata: {},
+      recordInputs: false,
+      recordOutputs: false
+    };
+  }
+}
+
+/**
+ * Initialize OpenTelemetry instrumentation
+ * Call this once at application startup
+ */
+export async function initializeTelemetry(): Promise<void> {
+  const telemetryEnabled = process.env.TELEMETRY_ENABLED === 'true';
+  
+  if (!telemetryEnabled) {
+    log.info('Telemetry disabled via environment variable');
+    return;
+  }
+  
+  try {
+    const settings = await getTelemetrySettings();
+    
+    if (!settings.enabled || !settings.endpoint) {
+      log.info('Telemetry disabled in settings or no endpoint configured');
+      return;
+    }
+    
+    // Note: OpenTelemetry dependencies not installed - placeholder implementation
+    log.info('OpenTelemetry packages not available, telemetry disabled');
+    return;
+    
+  } catch (error) {
+    log.error('Failed to initialize OpenTelemetry', {
+      error: error instanceof Error ? error.message : String(error)
+    });
+    // Don't throw - telemetry failures shouldn't break the app
+  }
+}
+
+/**
+ * Get telemetry settings from database with environment variable fallbacks
+ */
+async function getTelemetrySettings() {
+  try {
+    // Try to get settings from database first (if method exists)
+    // Type assertion for Settings method that may not exist
+    const settingsWithTelemetry = Settings as typeof Settings & {
+      getTelemetry?: () => Promise<{
+        enabled: boolean;
+        endpoint?: string;
+        headers?: Record<string, string>;
+        serviceName?: string;
+        serviceVersion?: string;
+        recordInputs?: boolean;
+        recordOutputs?: boolean;
+        recordUserContext?: boolean;
+        samplingRate?: number;
+      }>;
+    };
+    const dbSettings = settingsWithTelemetry.getTelemetry ? await settingsWithTelemetry.getTelemetry() : null;
+    
+    if (dbSettings) {
+      return dbSettings;
+    }
+  } catch {
+    log.debug('Could not load telemetry settings from database, using environment variables');
+  }
+  
+  // Fallback to environment variables
+  return {
+    enabled: process.env.TELEMETRY_ENABLED === 'true',
+    endpoint: process.env.TELEMETRY_ENDPOINT,
+    headers: process.env.TELEMETRY_HEADERS ? 
+      JSON.parse(process.env.TELEMETRY_HEADERS) : 
+      undefined,
+    serviceName: process.env.TELEMETRY_SERVICE_NAME || 'aistudio',
+    serviceVersion: process.env.TELEMETRY_SERVICE_VERSION || '1.0.0',
+    recordInputs: process.env.TELEMETRY_RECORD_INPUTS !== 'false', // Default true
+    recordOutputs: process.env.TELEMETRY_RECORD_OUTPUTS !== 'false', // Default true
+    recordUserContext: process.env.TELEMETRY_RECORD_USER_CONTEXT === 'true', // Default false
+    samplingRate: parseFloat(process.env.TELEMETRY_SAMPLING_RATE || '1.0')
+  };
+}
+
+/**
+ * Record custom metrics for AI operations
+ */
+export function recordAIMetrics(data: {
+  provider: string;
+  modelId: string;
+  source: string;
+  tokensInput?: number;
+  tokensOutput?: number;
+  tokensReasoning?: number;
+  cost?: number;
+  latencyMs?: number;
+  status: 'success' | 'error' | 'timeout';
+  errorType?: string;
+}) {
+  try {
+    const meter = metrics.getMeter('aistudio.ai', '1.0.0');
+    
+    // Token usage counters
+    if (data.tokensInput) {
+      meter.createCounter('ai.tokens.input', {
+        description: 'Input tokens used by AI models'
+      }).add(data.tokensInput, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source
+      });
+    }
+    
+    if (data.tokensOutput) {
+      meter.createCounter('ai.tokens.output', {
+        description: 'Output tokens generated by AI models'
+      }).add(data.tokensOutput, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source
+      });
+    }
+    
+    if (data.tokensReasoning) {
+      meter.createCounter('ai.tokens.reasoning', {
+        description: 'Reasoning tokens used by advanced AI models'
+      }).add(data.tokensReasoning, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source
+      });
+    }
+    
+    // Cost tracking
+    if (data.cost) {
+      meter.createCounter('ai.cost.total', {
+        description: 'Total cost of AI operations'
+      }).add(data.cost, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source
+      });
+    }
+    
+    // Latency histogram
+    if (data.latencyMs) {
+      meter.createHistogram('ai.request.duration', {
+        description: 'AI request duration in milliseconds'
+      }).record(data.latencyMs, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source,
+        status: data.status
+      });
+    }
+    
+    // Error tracking
+    if (data.status === 'error') {
+      meter.createCounter('ai.errors.total', {
+        description: 'Total AI operation errors'
+      }).add(1, {
+        provider: data.provider,
+        model: data.modelId,
+        source: data.source,
+        error_type: data.errorType || 'unknown'
+      });
+    }
+    
+    // Success rate
+    meter.createCounter('ai.requests.total', {
+      description: 'Total AI requests'
+    }).add(1, {
+      provider: data.provider,
+      model: data.modelId,
+      source: data.source,
+      status: data.status
+    });
+    
+  } catch (error) {
+    log.error('Failed to record AI metrics', {
+      error: error instanceof Error ? error.message : String(error),
+      data
+    });
+    // Don't throw - metrics failures shouldn't break AI operations
+  }
+}
\ No newline at end of file
diff --git a/lib/streaming/types.ts b/lib/streaming/types.ts
new file mode 100644
index 00000000..76cfb20d
--- /dev/null
+++ b/lib/streaming/types.ts
@@ -0,0 +1,276 @@
+import type { UIMessage, LanguageModel, CoreMessage } from 'ai';
+
+/**
+ * Core streaming types for the unified streaming architecture
+ */
+
+export interface StreamRequest {
+  // Core request data
+  messages: UIMessage[];
+  modelId: string;
+  provider: string;
+  
+  // User context
+  userId: string;
+  sessionId?: string;
+  conversationId?: string | number;
+  
+  // Request source and metadata
+  source: 'chat' | 'compare' | 'assistant_execution' | 'ai-helpers';
+  executionId?: number;
+  documentId?: string;
+  
+  // Model configuration
+  systemPrompt?: string;
+  maxTokens?: number;
+  temperature?: number;
+  timeout?: number;
+  
+  // Advanced model options
+  options?: {
+    // Reasoning configuration
+    reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high';
+    responseMode?: 'standard' | 'flex' | 'priority';
+    
+    // Background processing for long-running models
+    backgroundMode?: boolean;
+    
+    // Thinking configuration for Claude models
+    thinkingBudget?: number; // 1024-6553 tokens
+    
+    // Tool configuration
+    enableWebSearch?: boolean;
+    enableCodeInterpreter?: boolean;
+    enableImageGeneration?: boolean;
+  };
+  
+  // Telemetry configuration
+  telemetry?: {
+    recordInputs?: boolean;
+    recordOutputs?: boolean;
+    customAttributes?: Record<string, string | number | boolean>;
+  };
+  
+  // Callbacks for streaming events
+  callbacks?: StreamingCallbacks;
+}
+
+export interface StreamResponse {
+  result: {
+    toDataStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    usage: Promise<{
+      totalTokens?: number;
+      promptTokens?: number;
+      completionTokens?: number;
+      reasoningTokens?: number;
+      totalCost?: number;
+    }>;
+  };
+  requestId: string;
+  capabilities: ProviderCapabilities;
+  telemetryConfig: TelemetryConfig;
+}
+
+export interface StreamConfig {
+  model: LanguageModel;
+  messages: CoreMessage[];
+  system?: string;
+  maxTokens?: number;
+  temperature?: number;
+  timeout?: number;
+  providerOptions?: Record<string, unknown>;
+  experimental_telemetry?: {
+    isEnabled: boolean;
+    functionId: string;
+    metadata: Record<string, string | number | boolean>;
+    recordInputs: boolean;
+    recordOutputs: boolean;
+    tracer?: {
+      startSpan: (name: string, options?: Record<string, unknown>) => TelemetrySpan;
+    };
+  };
+}
+
+export interface ProviderCapabilities {
+  // Reasoning capabilities
+  supportsReasoning: boolean;
+  supportsThinking: boolean;
+  maxThinkingTokens?: number;
+  supportedResponseModes: ('standard' | 'flex' | 'priority')[];
+  
+  // Background processing
+  supportsBackgroundMode: boolean;
+  
+  // Built-in tools
+  supportedTools: string[];
+  
+  // Performance characteristics
+  typicalLatencyMs: number;
+  maxTimeoutMs: number;
+  
+  // Cost information
+  costPerInputToken?: number;
+  costPerOutputToken?: number;
+  costPerReasoningToken?: number;
+}
+
+export interface TelemetrySpan {
+  setAttributes: (attributes: Record<string, string | number | boolean>) => void;
+  addEvent: (name: string, attributes?: Record<string, unknown>) => void;
+  recordException: (error: Error) => void;
+  setStatus: (status: { code: number; message?: string }) => void;
+  end: () => void;
+}
+
+export interface TelemetryConfig {
+  isEnabled: boolean;
+  functionId: string;
+  metadata: Record<string, string | number | boolean>;
+  recordInputs: boolean;
+  recordOutputs: boolean;
+  tracer?: {
+    startSpan: (name: string, options?: Record<string, unknown>) => TelemetrySpan;
+  };
+}
+
+export interface StreamingProgress {
+  type: 'token' | 'reasoning' | 'thinking' | 'tool_call' | 'tool_result';
+  content?: string;
+  text?: string; // For token events
+  timestamp: number;
+  metadata?: Record<string, unknown>;
+}
+
+export interface StreamingCallbacks {
+  onProgress?: (event: StreamingProgress) => void;
+  onReasoning?: (reasoning: string) => void;
+  onThinking?: (thinking: string) => void;
+  onFinish?: (data: {
+    text: string;
+    usage?: {
+      promptTokens: number;
+      completionTokens: number;
+      totalTokens: number;
+      reasoningTokens?: number;
+      totalCost?: number;
+    };
+    finishReason: string;
+  }) => void;
+  onError?: (error: Error) => void;
+}
+
+export interface ProviderAdapter {
+  /**
+   * Create a model instance for this provider
+   */
+  createModel(modelId: string, options?: StreamRequest['options']): Promise<LanguageModel>;
+  
+  /**
+   * Get capabilities for a specific model
+   */
+  getCapabilities(modelId: string): ProviderCapabilities;
+  
+  /**
+   * Get provider-specific options for streaming
+   */
+  getProviderOptions(modelId: string, options?: StreamRequest['options']): Record<string, unknown>;
+  
+  /**
+   * Stream with provider-specific enhancements
+   */
+  streamWithEnhancements(
+    config: StreamConfig,
+    callbacks: StreamingCallbacks
+  ): Promise<{
+    toDataStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    toUIMessageStreamResponse: (options?: { headers?: Record<string, string> }) => Response;
+    usage: Promise<{
+      totalTokens?: number;
+      promptTokens?: number;
+      completionTokens?: number;
+      reasoningTokens?: number;
+      totalCost?: number;
+    }>;
+  }>;
+  
+  /**
+   * Validate if this adapter supports the given model
+   */
+  supportsModel(modelId: string): boolean;
+}
+
+// Export unified streaming service interface
+export interface IUnifiedStreamingService {
+  stream(request: StreamRequest): Promise<StreamResponse>;
+}
+
+// Error types specific to streaming
+export class StreamingError extends Error {
+  constructor(
+    message: string,
+    public code: string,
+    public provider?: string,
+    public modelId?: string,
+    public cause?: Error
+  ) {
+    super(message);
+    this.name = 'StreamingError';
+  }
+}
+
+export class ProviderUnavailableError extends StreamingError {
+  constructor(provider: string, cause?: Error) {
+    super(
+      `Provider ${provider} is currently unavailable`,
+      'PROVIDER_UNAVAILABLE',
+      provider,
+      undefined,
+      cause
+    );
+  }
+}
+
+export class ModelNotSupportedError extends StreamingError {
+  constructor(provider: string, modelId: string) {
+    super(
+      `Model ${modelId} is not supported by provider ${provider}`,
+      'MODEL_NOT_SUPPORTED',
+      provider,
+      modelId
+    );
+  }
+}
+
+export class StreamTimeoutError extends StreamingError {
+  constructor(timeoutMs: number, provider: string, modelId: string) {
+    super(
+      `Stream timed out after ${timeoutMs}ms`,
+      'STREAM_TIMEOUT',
+      provider,
+      modelId
+    );
+  }
+}
+
+// Utility types for frontend hooks
+export interface UseUnifiedStreamConfig {
+  source: StreamRequest['source'];
+  modelId?: string;
+  provider?: string;
+  systemPrompt?: string;
+  options?: StreamRequest['options'];
+  telemetry?: StreamRequest['telemetry'];
+}
+
+export interface UseUnifiedStreamReturn {
+  messages: UIMessage[];
+  status: 'idle' | 'loading' | 'streaming' | 'success' | 'error';
+  error: Error | null;
+  reasoning: string | null;
+  thinking: string | null;
+  sendMessage: (message: UIMessage, config?: Partial<StreamRequest>) => Promise<void>;
+  stop: () => void;
+  clear: () => void;
+  capabilities: ProviderCapabilities | null;
+}
\ No newline at end of file
diff --git a/lib/streaming/unified-streaming-service.ts b/lib/streaming/unified-streaming-service.ts
new file mode 100644
index 00000000..4bd124b8
--- /dev/null
+++ b/lib/streaming/unified-streaming-service.ts
@@ -0,0 +1,317 @@
+import { convertToModelMessages } from 'ai';
+import { createLogger, generateRequestId, startTimer } from '@/lib/logger';
+import { getTelemetryConfig } from './telemetry-service';
+import { getProviderAdapter, type ProviderCapabilities } from './provider-adapters';
+import { CircuitBreaker, CircuitBreakerOpenError } from './circuit-breaker';
+import type { StreamRequest, StreamResponse, StreamConfig, StreamingProgress, TelemetrySpan, TelemetryConfig } from './types';
+
+/**
+ * Unified streaming service that handles all AI streaming operations
+ * across chat, compare, and assistant execution tools.
+ * 
+ * Features:
+ * - Provider-specific optimizations (OpenAI Responses API, Claude thinking, etc.)
+ * - Comprehensive telemetry and observability
+ * - Circuit breaker pattern for reliability
+ * - Reasoning content extraction for advanced models
+ * - Adaptive timeouts based on model capabilities
+ */
+export class UnifiedStreamingService {
+  private circuitBreakers = new Map<string, CircuitBreaker>();
+  
+  /**
+   * Main streaming method that handles all AI operations
+   */
+  async stream(request: StreamRequest): Promise<StreamResponse> {
+    const requestId = generateRequestId();
+    const timer = startTimer('unified-streaming-service.stream');
+    const log = createLogger({ requestId, module: 'unified-streaming-service' });
+    
+    log.info('Starting unified stream', {
+      provider: request.provider,
+      modelId: request.modelId,
+      source: request.source,
+      userId: request.userId,
+      messageCount: request.messages.length
+    });
+    
+    try {
+      // 1. Get provider adapter and capabilities
+      const adapter = await getProviderAdapter(request.provider);
+      const capabilities = adapter.getCapabilities(request.modelId);
+      
+      // 2. Configure telemetry
+      const telemetryConfig = await getTelemetryConfig({
+        functionId: `${request.source}.stream`,
+        userId: request.userId,
+        sessionId: request.sessionId,
+        conversationId: request.conversationId,
+        modelId: request.modelId,
+        provider: request.provider,
+        source: request.source,
+        recordInputs: request.telemetry?.recordInputs,
+        recordOutputs: request.telemetry?.recordOutputs
+      });
+      
+      // 3. Check circuit breaker
+      const circuitBreaker = this.getCircuitBreaker(request.provider);
+      const circuitState = circuitBreaker.getState();
+      log.info('Circuit breaker state', {
+        provider: request.provider,
+        state: circuitState,
+        isOpen: circuitBreaker.isOpen(),
+        metrics: circuitBreaker.getMetrics()
+      });
+      
+      if (circuitBreaker.isOpen()) {
+        log.error('Circuit breaker is open, blocking request', {
+          provider: request.provider,
+          state: circuitState
+        });
+        throw new CircuitBreakerOpenError(request.provider, circuitState);
+      }
+      
+      // 4. Configure streaming with adaptive timeouts
+      const config: StreamConfig = {
+        model: await adapter.createModel(request.modelId, request.options),
+        messages: convertToModelMessages(request.messages),
+        system: request.systemPrompt,
+        maxTokens: request.maxTokens,
+        temperature: request.temperature,
+        // Adaptive timeout based on model capabilities
+        timeout: this.getAdaptiveTimeout(capabilities, request),
+        // Provider-specific options
+        providerOptions: adapter.getProviderOptions(request.modelId, request.options),
+        // Telemetry configuration
+        experimental_telemetry: telemetryConfig.isEnabled ? {
+          isEnabled: true,
+          functionId: telemetryConfig.functionId,
+          metadata: telemetryConfig.metadata,
+          recordInputs: telemetryConfig.recordInputs,
+          recordOutputs: telemetryConfig.recordOutputs,
+          tracer: telemetryConfig.tracer
+        } : undefined
+      };
+      
+      // 5. Start telemetry span
+      const span = telemetryConfig.tracer?.startSpan('ai.stream.unified', {
+        attributes: {
+          'ai.provider': request.provider,
+          'ai.model.id': request.modelId,
+          'ai.source': request.source,
+          'ai.reasoning.capable': capabilities.supportsReasoning,
+          'ai.thinking.capable': capabilities.supportsThinking,
+          'ai.request.timeout': config.timeout
+        }
+      });
+      
+      try {
+        // 6. Execute streaming with provider-specific handling
+        const result = await adapter.streamWithEnhancements(config, {
+          onProgress: (event) => {
+            this.handleProgress(event, span, telemetryConfig);
+            request.callbacks?.onProgress?.(event);
+          },
+          onReasoning: (reasoning) => {
+            this.handleReasoning(reasoning, span);
+            request.callbacks?.onReasoning?.(reasoning);
+          },
+          onThinking: (thinking) => {
+            this.handleThinking(thinking, span);
+            request.callbacks?.onThinking?.(thinking);
+          },
+          onFinish: async (data) => {
+            this.handleFinish(data, span, telemetryConfig, timer);
+            // Call user-provided onFinish callback
+            if (request.callbacks?.onFinish) {
+              try {
+                await request.callbacks.onFinish(data);
+              } catch (error) {
+                log.error('Critical: Failed to save assistant message', { 
+                  error,
+                  conversationId: request.conversationId,
+                  userId: request.userId
+                });
+                // Add telemetry for failed saves
+                if (span) {
+                  span.recordException(error as Error);
+                  span.setAttributes({
+                    'ai.message.save.failed': true,
+                    'ai.message.save.error': (error as Error).message
+                  });
+                }
+                // Don't rethrow to avoid breaking the stream, but mark as error
+                // The message is already displayed to user, just not persisted
+              }
+            }
+          },
+          onError: (error) => {
+            this.handleError(error, span, circuitBreaker);
+            request.callbacks?.onError?.(error);
+          }
+        });
+        
+        // 7. Mark circuit breaker as successful
+        circuitBreaker.recordSuccess();
+        
+        log.info('Stream completed successfully', {
+          provider: request.provider,
+          modelId: request.modelId,
+          source: request.source
+        });
+        
+        return {
+          result,
+          requestId,
+          capabilities,
+          telemetryConfig
+        };
+        
+      } catch (error) {
+        span?.recordException(error as Error);
+        span?.setStatus({ code: 2 }); // ERROR
+        circuitBreaker.recordFailure();
+        throw error;
+      } finally {
+        span?.end();
+      }
+      
+    } catch (error) {
+      timer({ status: 'error' });
+      log.error('Stream failed', {
+        error: error instanceof Error ? error.message : String(error),
+        provider: request.provider,
+        modelId: request.modelId,
+        source: request.source
+      });
+      throw error;
+    }
+  }
+  
+  /**
+   * Get or create circuit breaker for provider
+   */
+  private getCircuitBreaker(provider: string): CircuitBreaker {
+    if (!this.circuitBreakers.has(provider)) {
+      this.circuitBreakers.set(provider, new CircuitBreaker({
+        failureThreshold: 5,
+        recoveryTimeoutMs: 60000, // 1 minute
+        monitoringPeriodMs: 60000  // 1 minute
+      }));
+    }
+    return this.circuitBreakers.get(provider)!;
+  }
+  
+  /**
+   * Calculate adaptive timeout based on model capabilities and request
+   */
+  private getAdaptiveTimeout(capabilities: ProviderCapabilities, request: StreamRequest): number {
+    const baseTimeout = 30000; // 30 seconds
+    
+    // Extend timeout for reasoning models
+    if (capabilities.supportsReasoning) {
+      // o3/o4 models may need up to 5 minutes for complex reasoning
+      if (request.modelId.includes('o3') || request.modelId.includes('o4')) {
+        return 300000; // 5 minutes
+      }
+      // Claude thinking models may need up to 2 minutes
+      if (capabilities.supportsThinking) {
+        return 120000; // 2 minutes
+      }
+      // Other reasoning models get 1 minute
+      return 60000;
+    }
+    
+    // Standard models use base timeout
+    return request.timeout || baseTimeout;
+  }
+  
+  /**
+   * Handle streaming progress events
+   */
+  private handleProgress(event: StreamingProgress, span: TelemetrySpan | undefined, telemetryConfig: TelemetryConfig) {
+    // Record progress metrics
+    if (telemetryConfig.isEnabled && span) {
+      span.addEvent('ai.stream.progress', {
+        timestamp: Date.now(),
+        'ai.tokens.streamed': (event.metadata?.tokens as number) || 0
+      });
+    }
+  }
+  
+  /**
+   * Handle reasoning content for advanced models
+   */
+  private handleReasoning(reasoning: string, span: TelemetrySpan | undefined) {
+    if (span) {
+      span.addEvent('ai.reasoning.chunk', {
+        timestamp: Date.now(),
+        'ai.reasoning.length': reasoning.length
+      });
+    }
+  }
+  
+  /**
+   * Handle thinking content for Claude models
+   */
+  private handleThinking(thinking: string, span: TelemetrySpan | undefined) {
+    if (span) {
+      span.addEvent('ai.thinking.chunk', {
+        timestamp: Date.now(),
+        'ai.thinking.length': thinking.length
+      });
+    }
+  }
+  
+  /**
+   * Handle stream completion
+   */
+  private handleFinish(
+    data: {
+      text: string;
+      usage?: {
+        promptTokens: number;
+        completionTokens: number;
+        totalTokens: number;
+        reasoningTokens?: number;
+        totalCost?: number;
+      };
+      finishReason: string;
+    },
+    span: TelemetrySpan | undefined,
+    telemetryConfig: TelemetryConfig,
+    timer: (metadata?: Record<string, unknown>) => void
+  ) {
+    if (span) {
+      span.setAttributes({
+        'ai.tokens.input': data.usage?.promptTokens || 0,
+        'ai.tokens.output': data.usage?.completionTokens || 0,
+        'ai.tokens.total': data.usage?.totalTokens || 0,
+        'ai.tokens.reasoning': data.usage?.reasoningTokens || 0,
+        'ai.finish_reason': data.finishReason || 'unknown',
+        'ai.cost.total': data.usage?.totalCost || 0
+      });
+      span.setStatus({ code: 1 }); // OK
+    }
+    
+    timer({ 
+      status: 'success',
+      tokensUsed: data.usage?.totalTokens || 0,
+      finishReason: data.finishReason
+    });
+  }
+  
+  /**
+   * Handle stream errors
+   */
+  private handleError(error: Error, span: TelemetrySpan | undefined, circuitBreaker: CircuitBreaker) {
+    if (span) {
+      span.recordException(error);
+      span.setStatus({ code: 2 }); // ERROR
+    }
+    circuitBreaker.recordFailure();
+  }
+}
+
+// Singleton instance
+export const unifiedStreamingService = new UnifiedStreamingService();
\ No newline at end of file
